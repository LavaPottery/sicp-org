*** 5.5.5 An Example of Compiled Code
:properties:
:custom_id: section-5.5.5
:end:

Now that we have seen all the elements of the compiler, let us examine an example of compiled code to see how things fit together.  We will compile the definition of a recursive ~factorial~ procedure by calling ~compile~:

#+begin_src scheme
(compile
 '(define (factorial n)
    (if (= n 1)
        1
        (* (factorial (- n 1)) n)))
 'val
 'next)
#+end_src

We have specified that the value of the ~define~ expression should be placed in the ~val~ register.  We don't care what the compiled code does after executing the ~define~, so our choice of ~next~ as the linkage descriptor is arbitrary.

~compile~ determines that the expression is a definition, so it calls ~compile-definition~ to compile code to compute the value to be assigned (targeted to ~val~), followed by code to install the definition, followed by code to put the value of the ~define~ (which is the symbol ~ok~) into the target register, followed finally by the linkage code.  ~env~ is preserved around the computation of the value, because it is needed in order to install the definition.  Because the linkage is ~next~, there is no linkage code in this case.  The skeleton of the compiled code is thus

#+begin_src scheme
<save 'env' if modified by code to compute value>
  <compilation of definition value, target 'val', linkage 'next'>
  <restore 'env' if saved above>
  (perform (op define-variable!)
           (const factorial)
           (reg val)
           (reg env))
  (assign val (const ok))
#+end_src

The expression that is to be compiled to produce the value for the variable ~factorial~ is a ~lambda~ expression whose value is the procedure that computes factorials.  ~compile~ handles this by calling ~compile-lambda~, which compiles the procedure body, labels it as a new entry point, and generates the instruction that will combine the procedure body at the new entry point with the run-time environment and assign the result to ~val~.  The sequence then skips around the compiled procedure code, which is inserted at this point.  The procedure code itself begins by extending the procedure's definition environment by a frame that binds the formal parameter ~n~ to the procedure argument.  Then comes the actual procedure body.  Since this code for the value of the variable doesn't modify the ~env~ register, the optional ~save~ and ~restore~ shown above aren't generated.  (The procedure code at ~entry2~ isn't executed at this point, so its use of ~env~ is irrelevant.)  Therefore, the skeleton for the compiled code becomes

#+begin_src scheme
(assign val (op make-compiled-procedure)
        (label entry2)
        (reg env))
  (goto (label after-lambda1))
entry2
  (assign env (op compiled-procedure-env) (reg proc))
  (assign env (op extend-environment)
              (const (n))
              (reg argl)
              (reg env))
  <compilation of procedure body>
after-lambda1
  (perform (op define-variable!)
           (const factorial)
           (reg val)
           (reg env))
  (assign val (const ok))
#+end_src

A procedure body is always compiled (by ~compile-lambda-body~) as a sequence with target ~val~ and linkage ~return~.  The sequence in this case consists of a single ~if~ expression:

#+begin_src scheme
(if (= n 1)
    1
    (* (factorial (- n 1)) n))
#+end_src

~compile-if~ generates code that first computes the predicate (targeted to ~val~), then checks the result and branches around the true branch if the predicate is false.  ~env~ and ~continue~ are preserved around the predicate code, since they may be needed for the rest of the ~if~ expression.  Since the ~if~ expression is the final expression (and only expression) in the sequence making up the procedure body, its target is ~val~ and its linkage is ~return~, so the true and false branches are both compiled with target ~val~ and linkage ~return~.  (That is, the value of the conditional, which is the value computed by either of its branches, is the value of the procedure.)

#+begin_src scheme
<save 'continue', 'env' if modified by predicate and needed by branches>
  <compilation of predicate, target 'val', linkage 'next'>
  <restore 'continue', 'env' if saved above>
  (test (op false?) (reg val))
  (branch (label false-branch4))
true-branch5
  <compilation of true branch, target 'val', linkage 'return'>
false-branch4
  <compilation of false branch, target 'val', linkage 'return'>
after-if3
#+end_src

The predicate ~(= n 1)~ is a procedure call.  This looks up the operator (the symbol ~=~) and places this value in ~proc~.  It then assembles the arguments ~1~ and the value of ~n~ into ~argl~.  Then it tests whether ~proc~ contains a primitive or a compound procedure, and dispatches to a primitive branch or a compound branch accordingly.  Both branches resume at the ~after-call~ label.  The requirements to preserve registers around the evaluation of the operator and operands don't result in any saving of registers, because in this case those evaluations don't modify the registers in question.

#+begin_src scheme
(assign proc
        (op lookup-variable-value) (const =) (reg env))
  (assign val (const 1))
  (assign argl (op list) (reg val))
  (assign val (op lookup-variable-value) (const n) (reg env))
  (assign argl (op cons) (reg val) (reg argl))
  (test (op primitive-procedure?) (reg proc))
  (branch (label primitive-branch17))
compiled-branch16
  (assign continue (label after-call15))
  (assign val (op compiled-procedure-entry) (reg proc))
  (goto (reg val))
primitive-branch17
  (assign val (op apply-primitive-procedure)
              (reg proc)
              (reg argl))
after-call15
#+end_src

The true branch, which is the constant 1, compiles (with target ~val~ and linkage ~return~) to

#+begin_src scheme
(assign val (const 1))
  (goto (reg continue))
#+end_src

The code for the false branch is another a procedure call, where the procedure is the value of the symbol ~*~, and the arguments are ~n~ and the result of another procedure call (a call to ~factorial~).  Each of these calls sets up ~proc~ and ~argl~ and its own primitive and compound branches.  [[figure-5.17][Figure 5.17]] shows the complete compilation of the definition of the ~factorial~ procedure.  Notice that the possible ~save~ and ~restore~ of ~continue~ and ~env~ around the predicate, shown above, are in fact generated, because these registers are modified by the procedure call in the predicate and needed for the procedure call and the ~return~ linkage in the branches.

**** Exercise 5.33
:properties:
:custom_id: exercise-5.33
:end:

Consider the following definition of a factorial procedure, which is slightly different from the one given above:

#+begin_src scheme
(define (factorial-alt n)
  (if (= n 1)
      1
      (* n (factorial-alt (- n 1)))))
#+end_src

Compile this procedure and compare the resulting code with that produced for ~factorial~.  Explain any differences you find.  Does either program execute more efficiently than the other?

**** Exercise 5.34
:properties:
:custom_id: exercise-5.34
:end:

Compile the iterative factorial procedure

#+begin_src scheme
(define (factorial n)
  (define (iter product counter)
    (if (> counter n)
        product
        (iter (* counter product)
              (+ counter 1))))
  (iter 1 1))
#+end_src

Annotate the resulting code, showing the essential difference between the code for iterative and recursive versions of ~factorial~ that makes one process build up stack space and the other run in constant stack space.

<<figure-5.17>> Compilation of the definition of the ~factorial~ procedure

#+begin_src scheme
;; construct the procedure and skip over code for the procedure body
(assign val
        (op make-compiled-procedure) (label entry2) (reg env))
  (goto (label after-lambda1))

entry2     ; calls to ~factorial~ will enter here
  (assign env (op compiled-procedure-env) (reg proc))
  (assign env
          (op extend-environment) (const (n)) (reg argl) (reg env))
;; begin actual procedure body
  (save continue)
  (save env)

;; compute ~(= n 1)~
  (assign proc (op lookup-variable-value) (const =) (reg env))
  (assign val (const 1))
  (assign argl (op list) (reg val))
  (assign val (op lookup-variable-value) (const n) (reg env))
  (assign argl (op cons) (reg val) (reg argl))
  (test (op primitive-procedure?) (reg proc))
  (branch (label primitive-branch17))
compiled-branch16
  (assign continue (label after-call15))
  (assign val (op compiled-procedure-entry) (reg proc))
  (goto (reg val))
primitive-branch17
  (assign val (op apply-primitive-procedure) (reg proc) (reg argl))

after-call15   ; ~val~ now contains result of ~(= n 1)~
  (restore env)
  (restore continue)
  (test (op false?) (reg val))
  (branch (label false-branch4))
true-branch5  ; return 1
  (assign val (const 1))
  (goto (reg continue))

false-branch4
;; compute and return ~(* (factorial (- n 1)) n)~
  (assign proc (op lookup-variable-value) (const *) (reg env))
  (save continue)
  (save proc)   ; save ~*~ procedure
  (assign val (op lookup-variable-value) (const n) (reg env))
  (assign argl (op list) (reg val))
  (save argl)   ; save partial argument list for ~*~

;; compute ~(factorial (- n 1))~, which is the other argument for ~*~
  (assign proc
          (op lookup-variable-value) (const factorial) (reg env))
  (save proc)  ; save ~factorial~ procedure
#+end_src

#+begin_src scheme
;; compute ~(- n 1)~, which is the argument for ~factorial~
(assign proc (op lookup-variable-value) (const -) (reg env))
  (assign val (const 1))
  (assign argl (op list) (reg val))
  (assign val (op lookup-variable-value) (const n) (reg env))
  (assign argl (op cons) (reg val) (reg argl))
  (test (op primitive-procedure?) (reg proc))
  (branch (label primitive-branch8))
compiled-branch7
  (assign continue (label after-call6))
  (assign val (op compiled-procedure-entry) (reg proc))
  (goto (reg val))
primitive-branch8
  (assign val (op apply-primitive-procedure) (reg proc) (reg argl))

after-call6   ; ~val~ now contains result of ~(- n 1)~
  (assign argl (op list) (reg val))
  (restore proc) ; restore ~factorial~
;; apply ~factorial~
  (test (op primitive-procedure?) (reg proc))
  (branch (label primitive-branch11))
compiled-branch10
  (assign continue (label after-call9))
  (assign val (op compiled-procedure-entry) (reg proc))
  (goto (reg val))
primitive-branch11
  (assign val (op apply-primitive-procedure) (reg proc) (reg argl))

after-call9      ; ~val~ now contains result of ~(factorial (- n 1))~
  (restore argl) ; restore partial argument list for ~*~
  (assign argl (op cons) (reg val) (reg argl))
  (restore proc) ; restore ~*~
  (restore continue)
;; apply ~*~ and return its value
  (test (op primitive-procedure?) (reg proc))
  (branch (label primitive-branch14))
compiled-branch13
;; note that a compound procedure here is called tail-recursively
  (assign val (op compiled-procedure-entry) (reg proc))
  (goto (reg val))
primitive-branch14
  (assign val (op apply-primitive-procedure) (reg proc) (reg argl))
  (goto (reg continue))
after-call12
after-if3
after-lambda1
;; assign the procedure to the variable ~factorial~
  (perform
   (op define-variable!) (const factorial) (reg val) (reg env))
  (assign val (const ok))
#+end_src

**** Exercise 5.35
:properties:
:custom_id: exercise-5.35
:end:

What expression was compiled to produce the code shown in [[figure-5.18][Figure 5.18]]?

<<figure-5.18>> An example of compiler output.  See [[#exercise-5.35][Exercise 5.35]].

#+begin_src scheme
(assign val (op make-compiled-procedure) (label entry16)
        (reg env))
  (goto (label after-lambda15))
entry16
  (assign env (op compiled-procedure-env) (reg proc))
  (assign env
          (op extend-environment) (const (x)) (reg argl) (reg env))
  (assign proc (op lookup-variable-value) (const +) (reg env))
  (save continue)
  (save proc)
  (save env)
  (assign proc (op lookup-variable-value) (const g) (reg env))
  (save proc)
  (assign proc (op lookup-variable-value) (const +) (reg env))
  (assign val (const 2))
  (assign argl (op list) (reg val))
  (assign val (op lookup-variable-value) (const x) (reg env))
  (assign argl (op cons) (reg val) (reg argl))
  (test (op primitive-procedure?) (reg proc))
  (branch (label primitive-branch19))
compiled-branch18
  (assign continue (label after-call17))
  (assign val (op compiled-procedure-entry) (reg proc))
  (goto (reg val))
primitive-branch19
  (assign val (op apply-primitive-procedure) (reg proc) (reg argl))
after-call17
  (assign argl (op list) (reg val))
  (restore proc)
  (test (op primitive-procedure?) (reg proc))
  (branch (label primitive-branch22))
compiled-branch21
  (assign continue (label after-call20))
  (assign val (op compiled-procedure-entry) (reg proc))
  (goto (reg val))
primitive-branch22
  (assign val (op apply-primitive-procedure) (reg proc) (reg argl))
#+end_src

#+begin_src scheme
after-call20
  (assign argl (op list) (reg val))
  (restore env)
  (assign val (op lookup-variable-value) (const x) (reg env))
  (assign argl (op cons) (reg val) (reg argl))
  (restore proc)
  (restore continue)
  (test (op primitive-procedure?) (reg proc))
  (branch (label primitive-branch25))
compiled-branch24
  (assign val (op compiled-procedure-entry) (reg proc))
  (goto (reg val))
primitive-branch25
  (assign val (op apply-primitive-procedure) (reg proc) (reg argl))
  (goto (reg continue))
after-call23
after-lambda15
  (perform (op define-variable!) (const f) (reg val) (reg env))
  (assign val (const ok))
#+end_src

**** Exercise 5.36
:properties:
:custom_id: exercise-5.36
:end:

What order of evaluation does our compiler produce for operands of a combination?  Is it left-to-right, right-to-left, or some other order?  Where in the compiler is this order determined?  Modify the compiler so that it produces some other order of evaluation.  (See the discussion of order of evaluation for the explicit-control evaluator in section [[#section-5.4.1][5.4.1]].)  How does changing the order of operand evaluation affect the efficiency of the code that constructs the argument list?

**** Exercise 5.37
:properties:
:custom_id: exercise-5.37
:end:

One way to understand the compiler's ~preserving~ mechanism for optimizing stack usage is to see what extra operations would be generated if we did not use this idea.  Modify ~preserving~ so that it always generates the ~save~ and ~restore~ operations.  Compile some simple expressions and identify the unnecessary stack operations that are generated.  Compare the code to that generated with the ~preserving~ mechanism intact.

**** Exercise 5.38
:properties:
:custom_id: exercise-5.38
:end:

Our compiler is clever about avoiding unnecessary stack operations, but it is not clever at all when it comes to compiling calls to the primitive procedures of the language in terms of the primitive operations supplied by the machine.  For example, consider how much code is compiled to compute ~(+ a 1)~: The code sets up an argument list in ~argl~, puts the primitive addition procedure (which it finds by looking up the symbol ~+~ in the environment) into ~proc~, and tests whether the procedure is primitive or compound.  The compiler always generates code to perform the test, as well as code for primitive and compound branches (only one of which will be executed).  We have not shown the part of the controller that implements primitives, but we presume that these instructions make use of primitive arithmetic operations in the machine's data paths.  Consider how much less code would be generated if the compiler could <<i268>> open-code primitives--that is, if it could generate code to directly use these primitive machine operations.  The expression ~(+ a 1)~ might be compiled into something as simple as [fn:328]

#+begin_src scheme
(assign val (op lookup-variable-value) (const a) (reg env))
(assign val (op +) (reg val) (const 1))
#+end_src

In this exercise we will extend our compiler to support open coding of selected primitives.  Special-purpose code will be generated for calls to these primitive procedures instead of the general procedure-application code.  In order to support this, we will augment our machine with special argument registers ~arg1~ and ~arg2~.  The primitive arithmetic operations of the machine will take their inputs from ~arg1~ and ~arg2~.  The results may be put into ~val~, ~arg1~, or ~arg2~.

The compiler must be able to recognize the application of an open-coded primitive in the source program.  We will augment the dispatch in the ~compile~ procedure to recognize the names of these primitives in addition to the reserved words (the special forms) it currently recognizes.[fn:329] For each special form our compiler has a code generator.  In this exercise we will construct a family of code generators for the open-coded primitives.

a. The open-coded primitives, unlike the special forms, all need their operands evaluated.  Write a code generator ~spread-arguments~ for use by all the open-coding code generators.  ~spread-arguments~ should take an operand list and compile the given operands targeted to successive argument registers.  Note that an operand may contain a call to an open-coded primitive, so argument registers will have to be preserved during operand evaluation.

b. For each of the primitive procedures =, *, -, and +, write a code generator that takes a combination with that operator, together with a target and a linkage descriptor, and produces code to spread the arguments into the registers and then perform the operation targeted to the given target with the given linkage.  You need only handle expressions with two operands.  Make ~compile~ dispatch to these code generators.

c. Try your new compiler on the ~factorial~ example.  Compare the resulting code with the result produced without open coding.

d. Extend your code generators for ~+~ and ~*~ so that they can handle expressions with arbitrary numbers of operands.  An expression with more than two operands will have to be compiled into a sequence of operations, each with only two inputs.

*** 5.5.6 Lexical Addressing
:properties:
:custom_id: section-5.5.6
:end:

One of the most common optimizations performed by compilers is the optimization of variable lookup.  Our compiler, as we have implemented it so far, generates code that uses the ~lookup-variable-value~ operation of the evaluator machine.  This searches for a variable by comparing it with each variable that is currently bound, working frame by frame outward through the run-time environment.  This search can be expensive if the frames are deeply nested or if there are many variables.  For example, consider the problem of looking up the value of ~x~ while evaluating the expression ~(* x y z)~ in an application of the procedure that is returned by

#+begin_src scheme
(let ((x 3) (y 4))
  (lambda (a b c d e)
    (let ((y (* a b x))
          (z (+ c d x)))
      (* x y z))))
#+end_src

Since a ~let~ expression is just syntactic sugar for a ~lambda~ combination, this expression is equivalent to

#+begin_src scheme
((lambda (x y)
   (lambda (a b c d e)
     ((lambda (y z) (* x y z))
      (* a b x)
      (+ c d x))))
 3
 4)
#+end_src

Each time ~lookup-variable-value~ searches for ~x~, it must determine that the symbol ~x~ is not ~eq?~ to ~y~ or ~z~ (in the first frame), nor to ~a~, ~b~, ~c~, ~d~, or ~e~ (in the second frame).  We will assume, for the moment, that our programs do not use ~define~--that variables are bound only with ~lambda~.  Because our language is lexically scoped, the run-time environment for any expression will have a structure that parallels the lexical structure of the program in which the expression appears.[fn:330] Thus, the compiler can know, when it analyzes the above expression, that each time the procedure is applied the variable ~x~ in ~(* x y z)~ will be found two frames out from the current frame and will be the first variable in that frame.

We can exploit this fact by inventing a new kind of variable-lookup operation, ~lexical-address-lookup~, that takes as arguments an environment and a <<i204>> lexical address that consists of two numbers: a <<i146>> frame number, which specifies how many frames to pass over, and a <<i115>> displacement number, which specifies how many variables to pass over in that frame.  ~lexical-address-lookup~ will produce the value of the variable stored at that lexical address relative to the current environment.  If we add the ~lexical-address-lookup~ operation to our machine, we can make the compiler generate code that references variables using this operation, rather than ~lookup-variable-value~.  Similarly, our compiled code can use a new ~lexical-address-set!~ operation instead of ~set-variable-value!~.

In order to generate such code, the compiler must be able to determine the lexical address of a variable it is about to compile a reference to.  The lexical address of a variable in a program depends on where one is in the code.  For example, in the following program, the address of ~x~ in expression <E1> is (2,0)--two frames back and the first variable in the frame.  At that point ~y~ is at address (0,0) and ~c~ is at address (1,2).  In expression <E2>, ~x~ is at (1,0), ~y~ is at (1,1), and ~c~ is at (0,2).

#+begin_src scheme
((lambda (x y)
   (lambda (a b c d e)
     ((lambda (y z) <E1>)
      <E2>
      (+ c d x))))
 3
 4)
#+end_src

One way for the compiler to produce code that uses lexical addressing is to maintain a data structure called a <<i71>> compile-time environment.  This keeps track of which variables will be at which positions in which frames in the run-time environment when a particular variable-access operation is executed.  The compile-time environment is a list of frames, each containing a list of variables.  (There will of course be no values bound to the variables, since values are not computed at compile time.)  The compile-time environment becomes an additional argument to ~compile~ and is passed along to each code generator.  The top-level call to ~compile~ uses an empty compile-time environment.  When a ~lambda~ body is compiled, ~compile-lambda-body~ extends the compile-time environment by a frame containing the procedure's parameters, so that the sequence making up the body is compiled with that extended environment.  At each point in the compilation, ~compile-variable~ and ~compile-assignment~ use the compile-time environment in order to generate the appropriate lexical addresses.

[[#exercise-5.39][Exercise 5.39]] through [[#exercise-5.43][Exercise 5.43]] describe how to complete this sketch of the lexical-addressing strategy in order to incorporate lexical lookup into the compiler.  [[#exercise-5.44][Exercise 5.44]] describes another use for the compile-time environment.

**** Exercise 5.39
:properties:
:custom_id: exercise-5.39
:end:

Write a procedure ~lexical-address-lookup~ that implements the new lookup operation.  It should take two arguments--a lexical address and a run-time environment--and return the value of the variable stored at the specified lexical address.  ~lexical-address-lookup~ should signal an error if the value of the variable is the symbol ~*unassigned*~.[fn:331] Also write a procedure ~lexical-address-set!~ that implements the operation that changes the value of the variable at a specified lexical address.

**** Exercise 5.40
:properties:
:custom_id: exercise-5.40
:end:

Modify the compiler to maintain the compile-time environment as described above.  That is, add a compile-time-environment argument to ~compile~ and the various code generators, and extend it in ~compile-lambda-body~.

**** Exercise 5.41
:properties:
:custom_id: exercise-5.41
:end:

Write a procedure ~find-variable~ that takes as arguments a variable and a compile-time environment and returns the lexical address of the variable with respect to that environment.  For example, in the program fragment that is shown above, the compile-time environment during the compilation of expression <E1> is ~((y z) (a b c d e) (x y))~.  ~find-variable~ should produce

#+begin_src scheme
(find-variable 'c '((y z) (a b c d e) (x y)))
(1 2)

(find-variable 'x '((y z) (a b c d e) (x y)))
(2 0)

(find-variable 'w '((y z) (a b c d e) (x y)))
not-found
#+end_src

**** Exercise 5.42
:properties:
:custom_id: exercise-5.42
:end:

Using ~find-variable~ from [[#exercise-5.41][Exercise 5.41]], rewrite ~compile-variable~ and ~compile-assignment~ to output lexical-address instructions.  In cases where ~find-variable~ returns ~not-found~ (that is, where the variable is not in the compile-time environment), you should have the code generators use the evaluator operations, as before, to search for the binding.  (The only place a variable that is not found at compile time can be is in the global environment, which is part of the run-time environment but is not part of the compile-time environment.[fn:332] Thus, if you wish, you may have the evaluator operations look directly in the global environment, which can be obtained with the operation ~(op get-global-environment)~, instead of having them search the whole run-time environment found in ~env~.)  Test the modified compiler on a few simple cases, such as the nested ~lambda~ combination at the beginning of this section.

**** Exercise 5.43
:properties:
:custom_id: exercise-5.43
:end:

We argued in section [[#section-4.1.6][4.1.6]] that internal definitions for block structure should not be considered "real" 'define's.  Rather, a procedure body should be interpreted as if the internal variables being defined were installed as ordinary ~lambda~ variables initialized to their correct values using ~set!~.  Section [[#section-4.1.6][4.1.6]] and [[#exercise-4.16][Exercise 4.16]] showed how to modify the metacircular interpreter to accomplish this by scanning out internal definitions.  Modify the compiler to perform the same transformation before it compiles a procedure body.

**** Exercise 5.44
:properties:
:custom_id: exercise-5.44
:end:

In this section we have focused on the use of the compile-time environment to produce lexical addresses.  But there are other uses for compile-time environments.  For instance, in [[#exercise-5.38][Exercise 5.38]] we increased the efficiency of compiled code by open-coding primitive procedures.  Our implementation treated the names of open-coded procedures as reserved words.  If a program were to rebind such a name, the mechanism described in [[#exercise-5.38][Exercise 5.38]] would still open-code it as a primitive, ignoring the new binding.  For example, consider the procedure

#+begin_src scheme
(lambda (+ * a b x y)
  (+ (* a x) (* b y)))
#+end_src

which computes a linear combination of ~x~ and ~y~.  We might call it with arguments ~+matrix~, ~*matrix~, and four matrices, but the open-coding compiler would still open-code the ~+~ and the ~*~ in ~(+ (* a x) (* b y))~ as primitive ~+~ and ~*~.  Modify the open-coding compiler to consult the compile-time environment in order to compile the correct code for expressions involving the names of primitive procedures.  (The code will work correctly as long as the program does not ~define~ or ~set!~ these names.)

*** 5.5.7 Interfacing Compiled Code to the Evaluator
:properties:
:custom_id: section-5.5.7
:end:

We have not yet explained how to load compiled code into the evaluator machine or how to run it.  We will assume that the explicit-control-evaluator machine has been defined as in section [[#section-5.4.4][5.4.4]], with the additional operations specified in footnote [fn:38].  We will implement a procedure ~compile-and-go~ that compiles a Scheme expression, loads the resulting object code into the evaluator machine, and causes the machine to run the code in the evaluator global environment, print the result, and enter the evaluator's driver loop.  We will also modify the evaluator so that interpreted expressions can call compiled procedures as well as interpreted ones.  We can then put a compiled procedure into the machine and use the evaluator to call it:

#+begin_src scheme
(compile-and-go
 '(define (factorial n)
    (if (= n 1)
        1
        (* (factorial (- n 1)) n))))
 ;;; EC-Eval value:
ok

 ;;; EC-Eval input:
(factorial 5)
;;; EC-Eval value:
120
#+end_src

To allow the evaluator to handle compiled procedures (for example, to evaluate the call to ~factorial~ above), we need to change the code at ~apply-dispatch~ (section [[#section-5.4.1][5.4.1]]) so that it recognizes compiled procedures (as distinct from compound or primitive procedures) and transfers control directly to the entry point of the compiled code:[fn:333]

#+begin_src scheme
apply-dispatch
  (test (op primitive-procedure?) (reg proc))
  (branch (label primitive-apply))
  (test (op compound-procedure?) (reg proc))
  (branch (label compound-apply))
  (test (op compiled-procedure?) (reg proc))
  (branch (label compiled-apply))
  (goto (label unknown-procedure-type))

compiled-apply
  (restore continue)
  (assign val (op compiled-procedure-entry) (reg proc))
  (goto (reg val))
#+end_src

Note the restore of ~continue~ at ~compiled-apply~.  Recall that the evaluator was arranged so that at ~apply-dispatch~, the continuation would be at the top of the stack.  The compiled code entry point, on the other hand, expects the continuation to be in ~continue~, so ~continue~ must be restored before the compiled code is executed.

To enable us to run some compiled code when we start the evaluator machine, we add a ~branch~ instruction at the beginning of the evaluator machine, which causes the machine to go to a new entry point if the ~flag~ register is set.[fn:334]

#+begin_src scheme
(branch (label external-entry)) ; branches if ~flag~ is set
read-eval-print-loop
  (perform (op initialize-stack))
  ...
#+end_src

~external-entry~ assumes that the machine is started with ~val~ containing the location of an instruction sequence that puts a result into ~val~ and ends with ~(goto (reg continue))~.  Starting at this entry point jumps to the location designated by ~val~, but first assigns ~continue~ so that execution will return to ~print-result~, which prints the value in ~val~ and then goes to the beginning of the evaluator's read-eval-print loop.[fn:335]

#+begin_src scheme
external-entry
  (perform (op initialize-stack))
  (assign env (op get-global-environment))
  (assign continue (label print-result))
  (goto (reg val))
#+end_src

Now we can use the following procedure to compile a procedure definition, execute the compiled code, and run the read-eval-print loop so we can try the procedure.  Because we want the compiled code to return to the location in ~continue~ with its result in ~val~, we compile the expression with a target of ~val~ and a linkage of ~return~.  In order to transform the object code produced by the compiler into executable instructions for the evaluator register machine, we use the procedure ~assemble~ from the register-machine simulator (section [[#section-5.2.2][5.2.2]]).  We then initialize the ~val~ register to point to the list of instructions, set the ~flag~ so that the evaluator will go to ~external-entry~, and start the evaluator.

#+begin_src scheme
(define (compile-and-go expression)
  (let ((instructions
         (assemble (statements
                    (compile expression 'val 'return))
                   eceval)))
    (set! the-global-environment (setup-environment))
    (set-register-contents! eceval 'val instructions)
    (set-register-contents! eceval 'flag true)
    (start eceval)))
#+end_src

If we have set up stack monitoring, as at the end of section [[#section-5.4.4][5.4.4]], we can examine the stack usage of compiled code:

#+begin_src scheme
(compile-and-go
 '(define (factorial n)
    (if (= n 1)
        1
        (* (factorial (- n 1)) n))))

(total-pushes = 0 maximum-depth = 0)
;;; EC-Eval value:
ok

;;; EC-Eval input:
(factorial 5)
(total-pushes = 31 maximum-depth = 14)
;;; EC-Eval value:
120
#+end_src

Compare this example with the evaluation of ~(factorial 5)~ using the interpreted version of the same procedure, shown at the end of section [[#section-5.4.4][5.4.4]].  The interpreted version required 144 pushes and a maximum stack depth of 28.  This illustrates the optimization that results from our compilation strategy.

*Interpretation and compilation*

With the programs in this section, we can now experiment with the alternative execution strategies of interpretation and compilation.[fn:336] An interpreter raises the machine to the level of the user program; a compiler lowers the user program to the level of the machine language.  We can regard the Scheme language (or any programming language) as a coherent family of abstractions erected on the machine language.  Interpreters are good for interactive program development and debugging because the steps of program execution are organized in terms of these abstractions, and are therefore more intelligible to the programmer.  Compiled code can execute faster, because the steps of program execution are organized in terms of the machine language, and the compiler is free to make optimizations that cut across the higher-level abstractions.[fn:337]

The alternatives of interpretation and compilation also lead to different strategies for porting languages to new computers.  Suppose that we wish to implement Lisp for a new machine.  One strategy is to begin with the explicit-control evaluator of section [[#section-5.4][5.4]] and translate its instructions to instructions for the new machine.  A different strategy is to begin with the compiler and change the code generators so that they generate code for the new machine.  The second strategy allows us to run any Lisp program on the new machine by first compiling it with the compiler running on our original Lisp system, and linking it with a compiled version of the run-time library.[fn:338] Better yet, we can compile the compiler itself, and run this on the new machine to compile other Lisp programs.[fn:339] Or we can compile one of the interpreters of section [[#section-4.1][4.1]] to produce an interpreter that runs on the new machine.

**** Exercise 5.45
:properties:
:custom_id: exercise-5.45
:end:

By comparing the stack operations used by compiled code to the stack operations used by the evaluator for the same computation, we can determine the extent to which the compiler optimizes use of the stack, both in speed (reducing the total number of stack operations) and in space (reducing the maximum stack depth).  Comparing this optimized stack use to the performance of a special-purpose machine for the same computation gives some indication of the quality of the compiler.

a. [[#exercise-5.27][Exercise 5.27]] asked you to determine, as a function of n, the number of pushes and the maximum stack depth needed by the evaluator to compute n!  using the recursive factorial procedure given above.  [[#exercise-5.14][Exercise 5.14]] asked you to do the same measurements for the special-purpose factorial machine shown in [[figure-5.11][Figure 5.11]].  Now perform the same analysis using the compiled ~factorial~ procedure.

   Take the ratio of the number of pushes in the compiled version to the number of pushes in the interpreted version, and do the same for the maximum stack depth.  Since the number of operations and the stack depth used to compute n!  are linear in n, these ratios should approach constants as n becomes large.  What are these constants?  Similarly, find the ratios of the stack usage in the special-purpose machine to the usage in the interpreted version.

   Compare the ratios for special-purpose versus interpreted code to the ratios for compiled versus interpreted code.  You should find that the special-purpose machine does much better than the compiled code, since the hand-tailored controller code should be much better than what is produced by our rudimentary general-purpose compiler.

b. Can you suggest improvements to the compiler that would help it generate code that would come closer in performance to the hand-tailored version?

**** Exercise 5.46
:properties:
:custom_id: exercise-5.46
:end:

Carry out an analysis like the one in [[#exercise-5.45][Exercise 5.45]] to determine the effectiveness of compiling the tree-recursive Fibonacci procedure

#+begin_src scheme
(define (fib n)
  (if (< n 2)
      n
      (+ (fib (- n 1)) (fib (- n 2)))))
#+end_src

compared to the effectiveness of using the special-purpose Fibonacci machine of [[figure-5.12][Figure 5.12]].  (For measurement of the interpreted performance, see [[#exercise-5.29][Exercise 5.29]].)  For Fibonacci, the time resource used is not linear in n; hence the ratios of stack operations will not approach a limiting value that is independent of n.

**** Exercise 5.47
:properties:
:custom_id: exercise-5.47
:end:

This section described how to modify the explicit-control evaluator so that interpreted code can call compiled procedures.  Show how to modify the compiler so that compiled procedures can call not only primitive procedures and compiled procedures, but interpreted procedures as well.  This requires modifying ~compile-procedure-call~ to handle the case of compound (interpreted) procedures.  Be sure to handle all the same ~target~ and ~linkage~ combinations as in ~compile-proc-appl~.  To do the actual procedure application, the code needs to jump to the evaluator's ~compound-apply~ entry point.  This label cannot be directly referenced in object code (since the assembler requires that all labels referenced by the code it is assembling be defined there), so we will add a register called ~compapp~ to the evaluator machine to hold this entry point, and add an instruction to initialize it:

#+begin_src scheme
(assign compapp (label compound-apply))
  (branch (label external-entry)) ; branches if ~flag~ is set
read-eval-print-loop
  ...
#+end_src

To test your code, start by defining a procedure ~f~ that calls a procedure ~g~.  Use ~compile-and-go~ to compile the definition of ~f~ and start the evaluator.  Now, typing at the evaluator, define ~g~ and try to call ~f~.

**** Exercise 5.48
:properties:
:custom_id: exercise-5.48
:end:

The ~compile-and-go~ interface implemented in this section is awkward, since the compiler can be called only once (when the evaluator machine is started).  Augment the compiler-interpreter interface by providing a ~compile-and-run~ primitive that can be called from within the explicit-control evaluator as follows:

#+begin_src scheme
;;; EC-Eval input:
(compile-and-run
 '(define (factorial n)
    (if (= n 1)
        1
        (* (factorial (- n 1)) n))))
;;; EC-Eval value:
ok

;;; EC-Eval input:
(factorial 5)
;;; EC-Eval value:
120
#+end_src

**** Exercise 5.49
:properties:
:custom_id: exercise-5.49
:end:

As an alternative to using the explicit-control evaluator's read-eval-print loop, design a register machine that performs a read-compile-execute-print loop.  That is, the machine should run a loop that reads an expression, compiles it, assembles and executes the resulting code, and prints the result.  This is easy to run in our simulated setup, since we can arrange to call the procedures ~compile~ and ~assemble~ as "register-machine operations."

**** Exercise 5.50
:properties:
:custom_id: exercise-5.50
:end:

Use the compiler to compile the metacircular evaluator of section [[#section-4.1][4.1]] and run this program using the register-machine simulator.  (To compile more than one definition at a time, you can package the definitions in a ~begin~.)  The resulting interpreter will run very slowly because of the multiple levels of interpretation, but getting all the details to work is an instructive exercise.

**** Exercise 5.51
:properties:
:custom_id: exercise-5.51
:end:

Develop a rudimentary implementation of Scheme in C (or some other low-level language of your choice) by translating the explicit-control evaluator of section [[#section-5.4][5.4]] into C. In order to run this code you will need to also provide appropriate storage-allocation routines and other run-time support.

**** Exercise 5.52
:properties:
:custom_id: exercise-5.52
:end:

As a counterpoint to exercise [[#exercise-5.51][Exercise 5.51]], modify the compiler so that it compiles Scheme procedures into sequences of C instructions.  Compile the metacircular evaluator of section [[#section-4.1][4.1]] to produce a Scheme interpreter written in C.
