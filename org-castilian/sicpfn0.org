* Footnotes

[fn:1] The 'Lisp 1 Programmer's Manual' appeared in 1960, and the 'Lisp 1.5 Programmer's Manual' (McCarthy 1965) was published in 1962.  The early history of Lisp is described in McCarthy 1978.

[fn:2] The two dialects in which most major Lisp programs of the 1970s were written are MacLisp (Moon 1978; Pitman 1983), developed at the MIT Project MAC, and Interlisp (Teitelman 1974), developed at Bolt Beranek and Newman Inc.  and the Xerox Palo Alto Research Center.  Portable Standard Lisp (Hearn 1969; Griss 1981) was a Lisp dialect designed to be easily portable between different machines.  MacLisp spawned a number of subdialects, such as Franz Lisp, which was developed at the University of California at Berkeley, and Zetalisp (Moon 1981), which was based on a special-purpose processor designed at the MIT Artificial Intelligence Laboratory to run Lisp very efficiently.  The Lisp dialect used in this book, called Scheme (Steele 1975), was invented in 1975 by Guy Lewis Steele Jr.  and Gerald Jay Sussman of the MIT Artificial Intelligence Laboratory and later reimplemented for instructional use at MIT.  Scheme became an IEEE standard in 1990 (IEEE 1990).  The Common Lisp dialect (Steele 1982, Steele 1990) was developed by the Lisp community to combine features from the earlier Lisp dialects to make an industrial standard for Lisp.  Common Lisp became an ANSI standard in 1994 (ANSI 1994).

[fn:3] One such special application was a breakthrough computation of scientific importance--an integration of the motion of the Solar System that extended previous results by nearly two orders of magnitude, and demonstrated that the dynamics of the Solar System is chaotic.  This computation was made possible by new integration algorithms, a special-purpose compiler, and a special-purpose computer all implemented with the aid of software tools written in Lisp (Abelson et al.  1992; Sussman and Wisdom 1992).

[fn:4] The characterization of numbers as "simple data" is a barefaced bluff.  In fact, the treatment of numbers is one of the trickiest and most confusing aspects of any programming language.  Some typical issues involved are these: Some computer systems distinguish <<i191>> integers, such as 2, from <<i321>> real numbers, such as 2.71.  Is the real number 2.00 different from the integer 2?  Are the arithmetic operations used for integers the same as the operations used for real numbers?  Does 6 divided by 2 produce 3, or 3.0?  How large a number can we represent?  How many decimal places of accuracy can we represent?  Is the range of integers the same as the range of real numbers?  Above and beyond these questions, of course, lies a collection of issues concerning roundoff and truncation errors - the entire science of numerical analysis.  Since our focus in this book is on large-scale program design rather than on numerical techniques, we are going to ignore these problems.  The numerical examples in this chapter will exhibit the usual roundoff behavior that one observes when using arithmetic operations that preserve a limited number of decimal places of accuracy in noninteger operations.

[fn:5] Throughout this book, when we wish to emphasize the distinction between the input typed by the user and the response printed by the interpreter, we will show the latter in slanted characters.

[fn:6] Lisp systems typically provide features to aid the user in formatting expressions.  Two especially useful features are one that automatically indents to the proper pretty-print position whenever a new line is started and one that highlights the matching left parenthesis whenever a right parenthesis is typed.

[fn:7] Lisp obeys the convention that every expression has a value.  This convention, together with the old reputation of Lisp as an inefficient language, is the source of the quip by Alan Perlis (paraphrasing Oscar Wilde) that "Lisp programmers know the value of everything but the cost of nothing."

[fn:8] In this book, we do not show the interpreter's response to evaluating definitions, since this is highly implementation-dependent.

[fn:9] [[#section-3][Chapter 3]] will show that this notion of environment is crucial, both for understanding how the interpreter works and for implementing interpreters.

[fn:10] It may seem strange that the evaluation rule says, as part of the first step, that we should evaluate the leftmost element of a combination, since at this point that can only be an operator such as ~+~ or ~*~ representing a built-in primitive procedure such as addition or multiplication.  We will see later that it is useful to be able to work with combinations whose operators are themselves compound expressions.

[fn:11] Special syntactic forms that are simply convenient alternative surface structures for things that can be written in more uniform ways are sometimes called <<i382>> syntactic sugar, to use a phrase coined by Peter Landin.  In comparison with users of other languages, Lisp programmers, as a rule, are less concerned with matters of syntax.  (By contrast, examine any Pascal manual and notice how much of it is devoted to descriptions of syntax.)  This disdain for syntax is due partly to the flexibility of Lisp, which makes it easy to change surface syntax, and partly to the observation that many "convenient" syntactic constructs, which make the language less uniform, end up causing more trouble than they are worth when programs become large and complex.  In the words of Alan Perlis, "Syntactic sugar causes cancer of the semicolon."

[fn:12] Observe that there are two different operations being combined here: we are creating the procedure, and we are giving it the name ~square~.  It is possible, indeed important, to be able to separate these two notions--to create procedures without naming them, and to give names to procedures that have already been created.  We will see how to do this in section [[#section-1.3.2][1.3.2]].

[fn:13] Throughout this book, we will describe the general syntax of expressions by using italic symbols delimited by angle brackets--e.g., <NAME>--to denote the "slots" in the expression to be filled in when such an expression is actually used.

[fn:14] More generally, the body of the procedure can be a sequence of expressions.  In this case, the interpreter evaluates each expression in the sequence in turn and returns the value of the final expression as the value of the procedure application.

[fn:15] Despite the simplicity of the substitution idea, it turns out to be surprisingly complicated to give a rigorous mathematical definition of the substitution process.  The problem arises from the possibility of confusion between the names used for the formal parameters of a procedure and the (possibly identical) names used in the expressions to which the procedure may be applied.  Indeed, there is a long history of erroneous definitions of <<i374>> substitution in the literature of logic and programming semantics.  See Stoy 1977 for a careful discussion of substitution.

[fn:16] In [[#section-3][Chapter 3]] we will introduce <<i368>> stream processing, which is a way of handling apparently "infinite" data structures by incorporating a limited form of normal-order evaluation.  In section [[#section-4.2][4.2]] we will modify the Scheme interpreter to produce a normal-order variant of Scheme.

[fn:17] "Interpreted as either true or false" means this: In Scheme, there are two distinguished values that are denoted by the constants ~#t~ and ~#f~.  When the interpreter checks a predicate's value, it interprets ~#f~ as false.  Any other value is treated as true.  (Thus, providing ~#t~ is logically unnecessary, but it is convenient.)  In this book we will use names ~true~ and ~false~, which are associated with the values ~#t~ and ~#f~ respectively.

[fn:18] ~Abs~ also uses the "minus" operator -, which, when used with a single operand, as in ~(- x)~, indicates negation.

[fn:19] A minor difference between ~if~ and ~cond~ is that the <E> part of each ~cond~ clause may be a sequence of expressions.  If the corresponding <P> is found to be true, the expressions <E> are evaluated in sequence and the value of the final expression in the sequence is returned as the value of the ~cond~.  In an ~if~ expression, however, the <CONSEQUENT> and <ALTERNATIVE> must be single expressions.

[fn:20] Declarative and imperative descriptions are intimately related, as indeed are mathematics and computer science.  For instance, to say that the answer produced by a program is "correct" is to make a declarative statement about the program.  There is a large amount of research aimed at establishing techniques for proving that programs are correct, and much of the technical difficulty of this subject has to do with negotiating the transition between imperative statements (from which programs are constructed) and declarative statements (which can be used to deduce things).  In a related vein, an important current area in programming-language design is the exploration of so-called very high-level languages, in which one actually programs in terms of declarative statements.  The idea is to make interpreters sophisticated enough so that, given "what is" knowledge specified by the programmer, they can generate "how to" knowledge automatically.  This cannot be done in general, but there are important areas where progress has been made.  We shall revisit this idea in [[#section-4][Chapter 4]].

[fn:21] This square-root algorithm is actually a special case of Newton's method, which is a general technique for finding roots of equations.  The square-root algorithm itself was developed by Heron of Alexandria in the first century A.D. We will see how to express the general Newton's method as a Lisp procedure in section [[#section-1.3.4][1.3.4]].

[fn:22] We will usually give predicates names ending with question marks, to help us remember that they are predicates.  This is just a stylistic convention.  As far as the interpreter is concerned, the question mark is just an ordinary character.

[fn:23] Observe that we express our initial guess as 1.0 rather than 1.  This would not make any difference in many Lisp implementations.  MIT Scheme, however, distinguishes between exact integers and decimal values, and dividing two integers produces a rational number rather than a decimal.  For example, dividing 10 by 6 yields 5/3, while dividing 10.0 by 6.0 yields 1.6666666666666667.  (We will learn how to implement arithmetic on rational numbers in section [[#section-2.1.1][2.1.1]].)  If we start with an initial guess of 1 in our square-root program, and x is an exact integer, all subsequent values produced in the square-root computation will be rational numbers rather than decimals.  Mixed operations on rational numbers and decimals always yield decimals, so starting with an initial guess of 1.0 forces all subsequent values to be decimals.

[fn:24] Readers who are worried about the efficiency issues involved in using procedure calls to implement iteration should note the remarks on "tail recursion" in section [[#section-1.2.1][1.2.1]].

[fn:25] It is not even clear which of these procedures is a more efficient implementation.  This depends upon the hardware available.  There are machines for which the "obvious" implementation is the less efficient one.  Consider a machine that has extensive tables of logarithms and antilogarithms stored in a very efficient manner.

[fn:26] The concept of consistent renaming is actually subtle and difficult to define formally.  Famous logicians have made embarrassing errors here.

[fn:27] Lexical scoping dictates that free variables in a procedure are taken to refer to bindings made by enclosing procedure definitions; that is, they are looked up in the environment in which the procedure was defined.  We will see how this works in detail in chapter 3 when we study environments and the detailed behavior of the interpreter.

[fn:28] Embedded definitions must come first in a procedure body.  The management is not responsible for the consequences of running programs that intertwine definition and use.

[fn:29] In a real program we would probably use the block structure introduced in the last section to hide the definition of ~fact-iter~:

#+begin_src scheme
(define (factorial n)
  (define (iter product counter)
    (if (> counter n)
        product
        (iter (* counter product)
              (+ counter 1))))
  (iter 1 1))
#+end_src

We avoided doing this here so as to minimize the number of things to think about at once.

[fn:30] When we discuss the implementation of procedures on register machines in [[#section-5][Chapter 5]], we will see that any iterative process can be realized "in hardware" as a machine that has a fixed set of registers and no auxiliary memory.  In contrast, realizing a recursive process requires a machine that uses an auxiliary data structure known as a <<i362>> stack.

[fn:31] Tail recursion has long been known as a compiler optimization trick.  A coherent semantic basis for tail recursion was provided by Carl Hewitt (1977), who explained it in terms of the "message-passing" model of computation that we shall discuss in [[#section-3][Chapter 3]].  Inspired by this, Gerald Jay Sussman and Guy Lewis Steele Jr.  (see Steele 1975) constructed a tail-recursive interpreter for Scheme.  Steele later showed how tail recursion is a consequence of the natural way to compile procedure calls (Steele 1977).  The IEEE standard for Scheme requires that Scheme implementations be tail-recursive.

[fn:32] An example of this was hinted at in section [[#section-1.1.3][1.1.3]]: The interpreter itself evaluates expressions using a tree-recursive process.

[fn:33] For example, work through in detail how the reduction rule applies to the problem of making change for 10 cents using pennies and nickels.

[fn:34] One approach to coping with redundant computations is to arrange matters so that we automatically construct a table of values as they are computed.  Each time we are asked to apply the procedure to some argument, we first look to see if the value is already stored in the table, in which case we avoid performing the redundant computation.  This strategy, known as <<i388>> tabulation or <<i230>> memoization, can be implemented in a straightforward way.  Tabulation can sometimes be used to transform processes that require an exponential number of steps (such as ~count-change~) into processes whose space and time requirements grow linearly with the input.  See [[#exercise-3.27][Exercise 3.27]].

[fn:35] The elements of Pascal's triangle are called the <<i39>> binomial coefficients, because the nth row consists of the coefficients of the terms in the expansion of (x + y)^n.  This pattern for computing the coefficients appeared in Blaise Pascal's 1653 seminal work on probability theory, 'Traite' du triangle arithme'tique'.  According to Knuth (1973), the same pattern appears in the 'Szu-yuen Yu"-chien' ("The Precious Mirror of the Four Elements"), published by the Chinese mathematician Chu Shih-chieh in 1303, in the works of the twelfth-century Persian poet and mathematician Omar Khayyam, and in the works of the twelfth-century Hindu mathematician Bha'scara A'cha'rya.

[fn:36] These statements mask a great deal of oversimplification.  For instance, if we count process steps as "machine operations" we are making the assumption that the number of machine operations needed to perform, say, a multiplication is independent of the size of the numbers to be multiplied, which is false if the numbers are sufficiently large.  Similar remarks hold for the estimates of space.  Like the design and description of a process, the analysis of a process can be carried out at various levels of abstraction.

[fn:37] More precisely, the number of multiplications required is equal to 1 less than the log base 2 of n plus the number of ones in the binary representation of n.  This total is always less than twice the log base 2 of n.  The arbitrary constants k_1 and k_2 in the definition of order notation imply that, for a logarithmic process, the base to which logarithms are taken does not matter, so all such processes are described as \theta(log n).

[fn:38] You may wonder why anyone would care about raising numbers to the 1000th power.  See section [[#section-1.2.6][1.2.6]].

[fn:39] This iterative algorithm is ancient.  It appears in the 'Chandah-sutra' by A'cha'rya Pingala, written before 200 B.C. See Knuth 1981, section 4.6.3, for a full discussion and analysis of this and other methods of exponentiation.

[fn:40] This algorithm, which is sometimes known as the "Russian peasant method" of multiplication, is ancient.  Examples of its use are found in the Rhind Papyrus, one of the two oldest mathematical documents in existence, written about 1700 B.C. (and copied from an even older document) by an Egyptian scribe named A'h-mose.

[fn:41] This exercise was suggested to us by Joe Stoy, based on an example in Kaldewaij 1990.

[fn:42] Euclid's Algorithm is so called because it appears in Euclid's 'Elements' (Book 7, ca.  300 B.C.).  According to Knuth (1973), it can be considered the oldest known nontrivial algorithm.  The ancient Egyptian method of multiplication ([[#exercise-1.18][Exercise 1.18]]) is surely older, but, as Knuth explains, Euclid's algorithm is the oldest known to have been presented as a general algorithm, rather than as a set of illustrative examples.

[fn:43] This theorem was proved in 1845 by Gabriel Lame', a French mathematician and engineer known chiefly for his contributions to mathematical physics.  To prove the theorem, we consider pairs (a_k ,b_k), where a_k>= b_k, for which Euclid's Algorithm terminates in k steps.  The proof is based on the claim that, if (a_(k+1), b_(k+1)) -> (a_k, b_k) -> (a_(k-1), b_(k-1)) are three successive pairs in the reduction process, then we must have b_(k+1)>= b_k + b_(k-1).  To verify the claim, consider that a reduction step is defined by applying the transformation a_(k-1) = b_k, b_(k-1) = remainder of a_k divided by b_k.  The second equation means that a_k = qb_k + b_(k-1) for some positive integer q.  And since q must be at least 1 we have a_k = qb_k + b_(k-1) >= b_k + b_(k-1).  But in the previous reduction step we have b_(k+1) = a_k.  Therefore, b_(k+1) = a_k>= b_k + b_(k-1).  This verifies the claim.  Now we can prove the theorem by induction on k, the number of steps that the algorithm requires to terminate.  The result is true for k = 1, since this merely requires that b be at least as large as Fib(1) = 1.  Now, assume that the result is true for all integers less than or equal to k and establish the result for k + 1.  Let (a_(k+1), b_(k+1)) -> (a_k, b_k) -> (a_k-1), b_(k-1)) be successive pairs in the reduction process.  By our induction hypotheses, we have b_(k-1) >= Fib(k - 1) and b_k >= Fib(k).  Thus, applying the claim we just proved together with the definition of the Fibonacci numbers gives b_(k+1) >= b_k + b_(k-1) >= Fib(k) + Fib(k - 1) = Fib(k + 1), which completes the proof of Lame's Theorem.

[fn:44] If d is a divisor of n, then so is n/d.  But d and n/d cannot both be greater than [sqrt](n).

[fn:45] Pierre de Fermat (1601-1665) is considered to be the founder of modern number theory.  He obtained many important number-theoretic results, but he usually announced just the results, without providing his proofs.  Fermat's Little Theorem was stated in a letter he wrote in 1640.  The first published proof was given by Euler in 1736 (and an earlier, identical proof was discovered in the unpublished manuscripts of Leibniz).  The most famous of Fermat's results--known as Fermat's Last Theorem--was jotted down in 1637 in his copy of the book 'Arithmetic' (by the third-century Greek mathematician Diophantus) with the remark "I have discovered a truly remarkable proof, but this margin is too small to contain it."  Finding a proof of Fermat's Last Theorem became one of the most famous challenges in number theory.  A complete solution was finally given in 1995 by Andrew Wiles of Princeton University.

[fn:46] The reduction steps in the cases where the exponent e is greater than 1 are based on the fact that, for any integers x, y, and m, we can find the remainder of x times y modulo m by computing separately the remainders of x modulo m and y modulo m, multiplying these, and then taking the remainder of the result modulo m.  For instance, in the case where e is even, we compute the remainder of b^(e/2) modulo m, square this, and take the remainder modulo m.  This technique is useful because it means we can perform our computation without ever having to deal with numbers much larger than m.  (Compare [[#exercise-1.25][Exercise 1.25]].)

[fn:47] Numbers that fool the Fermat test are called <<i54>> Carmichael numbers, and little is known about them other than that they are extremely rare.  There are 255 Carmichael numbers below 100,000,000.  The smallest few are 561, 1105, 1729, 2465, 2821, and 6601.  In testing primality of very large numbers chosen at random, the chance of stumbling upon a value that fools the Fermat test is less than the chance that cosmic radiation will cause the computer to make an error in carrying out a "correct" algorithm.  Considering an algorithm to be inadequate for the first reason but not for the second illustrates the difference between mathematics and engineering.

[fn:48] One of the most striking applications of probabilistic prime testing has been to the field of cryptography.  Although it is now computationally infeasible to factor an arbitrary 200-digit number, the primality of such a number can be checked in a few seconds with the Fermat test.  This fact forms the basis of a technique for constructing "unbreakable codes" suggested by Rivest, Shamir, and Adleman (1977).  The resulting <<i338>> RSA algorithm has become a widely used technique for enhancing the security of electronic communications.  Because of this and related developments, the study of prime numbers, once considered the epitome of a topic in "pure" mathematics to be studied only for its own sake, now turns out to have important practical applications to cryptography, electronic funds transfer, and information retrieval.

[fn:49] This series, usually written in the equivalent form (\pi/4) = 1 - (1/3) + (1/5) - (1/7) + ..., is due to Leibniz.  We'll see how to use this as the basis for some fancy numerical tricks in section [[#section-3.5.3][3.5.3]].

[fn:50] Notice that we have used block structure (section [[#section-1.1.8][1.1.8]]) to embed the definitions of ~pi-next~ and ~pi-term~ within ~pi-sum~, since these procedures are unlikely to be useful for any other purpose.  We will see how to get rid of them altogether in section [[#section-1.3.2][1.3.2]].

[fn:51] The intent of [[#exercise-1.31][Exercise 1.31]] through [[#exercise-1.33][Exercise 1.33]] is to demonstrate the expressive power that is attained by using an appropriate abstraction to consolidate many seemingly disparate operations.  However, though accumulation and filtering are elegant ideas, our hands are somewhat tied in using them at this point since we do not yet have data structures to provide suitable means of combination for these abstractions.  We will return to these ideas in section [[#section-2.2.3][2.2.3]] when we show how to use <<i348>> sequences as interfaces for combining filters and accumulators to build even more powerful abstractions.  We will see there how these methods really come into their own as a powerful and elegant approach to designing programs.

[fn:52] This formula was discovered by the seventeenth-century English mathematician John Wallis.

[fn:53] It would be clearer and less intimidating to people learning Lisp if a name more obvious than ~lambda~, such as ~make-procedure~, were used.  But the convention is firmly entrenched.  The notation is adopted from the [lambda] calculus, a mathematical formalism introduced by the mathematical logician Alonzo Church (1941).  Church developed the [lambda] calculus to provide a rigorous foundation for studying the notions of function and function application.  The [lambda] calculus has become a basic tool for mathematical investigations of the semantics of programming languages.

[fn:54] Understanding internal definitions well enough to be sure a program means what we intend it to mean requires a more elaborate model of the evaluation process than we have presented in this chapter.  The subtleties do not arise with internal definitions of procedures, however.  We will return to this issue in section [[#section-4.1.6][4.1.6]], after we learn more about evaluation.

[fn:55] We have used 0.001 as a representative "small" number to indicate a tolerance for the acceptable error in a calculation.  The appropriate tolerance for a real calculation depends upon the problem to be solved and the limitations of the computer and the algorithm.  This is often a very subtle consideration, requiring help from a numerical analyst or some other kind of magician.

[fn:56] This can be accomplished using ~error~, which takes as arguments a number of items that are printed as error messages.

[fn:57] Try this during a boring lecture: Set your calculator to radians mode and then repeatedly press the ~cos~ button until you obtain the fixed point.

[fn:58] |-> (pronounced "maps to") is the mathematician's way of writing ~lambda~.  y |-> x/y means ~(lambda(y) (/ x y))~, that is, the function whose value at y is x/y.

[fn:59] Observe that this is a combination whose operator is itself a combination.  [[#exercise-1.4][Exercise 1.4]] already demonstrated the ability to form such combinations, but that was only a toy example.  Here we begin to see the real need for such combinations--when applying a procedure that is obtained as the value returned by a higher-order procedure.

[fn:60] See [[#exercise-1.45][Exercise 1.45]] for a further generalization.

[fn:61] Elementary calculus books usually describe Newton's method in terms of the sequence of approximations x_(n+1) = x_n - g(x_n)/Dg(x_n).  Having language for talking about processes and using the idea of fixed points simplifies the description of the method.

[fn:62] Newton's method does not always converge to an answer, but it can be shown that in favorable cases each iteration doubles the number-of-digits accuracy of the approximation to the solution.  In such cases, Newton's method will converge much more rapidly than the half-interval method.

[fn:63] For finding square roots, Newton's method converges rapidly to the correct solution from any starting point.

[fn:64] The notion of first-class status of programming-language elements is due to the British computer scientist Christopher Strachey (1916-1975).

[fn:65] We'll see examples of this after we introduce data structures in [[#section-2][Chapter 2]].

[fn:66] The major implementation cost of first-class procedures is that allowing procedures to be returned as values requires reserving storage for a procedure's free variables even while the procedure is not executing.  In the Scheme implementation we will study in section [[#section-4.1][4.1]], these variables are stored in the procedure's environment.

[fn:67] The ability to directly manipulate procedures provides an analogous increase in the expressive power of a programming language.  For example, in section [[#section-1.3.1][1.3.1]] we introduced the ~sum~ procedure, which takes a procedure ~term~ as an argument and computes the sum of the values of ~term~ over some specified interval.  In order to define ~sum~, it is crucial that we be able to speak of a procedure such as ~term~ as an entity in its own right, without regard for how ~term~ might be expressed with more primitive operations.  Indeed, if we did not have the notion of "a procedure," it is doubtful that we would ever even think of the possibility of defining an operation such as ~sum~.  Moreover, insofar as performing the summation is concerned, the details of how ~term~ may be constructed from more primitive operations are irrelevant.

[fn:68] The name ~cons~ stands for "construct."  The names ~car~ and ~cdr~ derive from the original implementation of Lisp on the IBM 704.  That machine had an addressing scheme that allowed one to reference the "address" and "decrement" parts of a memory location.  ~car~ stands for "Contents of Address part of Register" and ~cdr~ (pronounced "could-er") stands for "Contents of Decrement part of Register."

[fn:69] Another way to define the selectors and constructor is

#+begin_src scheme
(define make-rat cons)
(define numer car)
(define denom cdr)
#+end_src

The first definition associates the name ~make-rat~ with the value of the expression ~cons~, which is the primitive procedure that constructs pairs.  Thus ~make-rat~ and ~cons~ are names for the same primitive constructor.

Defining selectors and constructors in this way is efficient: Instead of ~make-rat~ /calling/ ~cons~, ~make-rat~ /is/ ~cons~, so there is only one procedure called, not two, when ~make-rat~ is called.  On the other hand, doing this defeats debugging aids that trace procedure calls or put breakpoints on procedure calls: You may want to watch ~make-rat~ being called, but you certainly don't want to watch every call to ~cons~.

We have chosen not to use this style of definition in this book.

[fn:70] ~Display~ is the Scheme primitive for printing data.  The Scheme primitive ~newline~ starts a new line for printing.  Neither of these procedures returns a useful value, so in the uses of ~print-rat~ below, we show only what ~print-rat~ prints, not what the interpreter prints as the value returned by ~print-rat~.

[fn:71] Surprisingly, this idea is very difficult to formulate rigorously.  There are two approaches to giving such a formulation.  One, pioneered by C. A.  R. Hoare (1972), is known as the method of <<i1>> abstract models.  It formalizes the "procedures plus conditions" specification as outlined in the rational-number example above.  Note that the condition on the rational-number representation was stated in terms of facts about integers (equality and division).  In general, abstract models define new kinds of data objects in terms of previously defined types of data objects.  Assertions about data objects can therefore be checked by reducing them to assertions about previously defined data objects.  Another approach, introduced by Zilles at MIT, by Goguen, Thatcher, Wagner, and Wright at IBM (see Thatcher, Wagner, and Wright 1978), and by Guttag at Toronto (see Guttag 1977), is called <<i15>> algebraic specification.  It regards the "procedures" as elements of an abstract algebraic system whose behavior is specified by axioms that correspond to our "conditions," and uses the techniques of abstract algebra to check assertions about data objects.  Both methods are surveyed in the paper by Liskov and Zilles (1975).

[fn:72] The use of the word "closure" here comes from abstract algebra, where a set of elements is said to be closed under an operation if applying the operation to elements in the set produces an element that is again an element of the set.  The Lisp community also (unfortunately) uses the word "closure" to describe a totally unrelated concept: A closure is an implementation technique for representing procedures with free variables.  We do not use the word "closure" in this second sense in this book.

[fn:73] The notion that a means of combination should satisfy closure is a straightforward idea.  Unfortunately, the data combiners provided in many popular programming languages do not satisfy closure, or make closure cumbersome to exploit.  In Fortran or Basic, one typically combines data elements by assembling them into arrays--but one cannot form arrays whose elements are themselves arrays.  Pascal and C admit structures whose elements are structures.  However, this requires that the programmer manipulate pointers explicitly, and adhere to the restriction that each field of a structure can contain only elements of a prespecified form.  Unlike Lisp with its pairs, these languages have no built-in general-purpose glue that makes it easy to manipulate compound data in a uniform way.  This limitation lies behind Alan Perlis's comment in his foreword to this book: "In Pascal the plethora of declarable data structures induces a specialization within functions that inhibits and penalizes casual cooperation.  It is better to have 100 functions operate on one data structure than to have 10 functions operate on 10 data structures."

[fn:74] In this book, we use <<i212>> list to mean a chain of pairs terminated by the end-of-list marker.  In contrast, the term <<i213>> list structure refers to any data structure made out of pairs, not just to lists.

[fn:75] Since nested applications of ~car~ and ~cdr~ are cumbersome to write, Lisp dialects provide abbreviations for them--for instance,

#+begin_example
(cadr (ARG)) = (car (cdr (ARG)))
#+end_example

The names of all such procedures start with ~c~ and end with ~r~.  Each ~a~ between them stands for a ~car~ operation and each ~d~ for a ~cdr~ operation, to be applied in the same order in which they appear in the name.  The names ~car~ and ~cdr~ persist because simple combinations like ~cadr~ are pronounceable.

[fn:76] It's remarkable how much energy in the standardization of Lisp dialects has been dissipated in arguments that are literally over nothing: Should ~nil~ be an ordinary name?  Should the value of ~nil~ be a symbol?  Should it be a list?  Should it be a pair?  In Scheme, ~nil~ is an ordinary name, which we use in this section as a variable whose value is the end-of-list marker (just as ~true~ is an ordinary variable that has a true value).  Other dialects of Lisp, including Common Lisp, treat ~nil~ as a special symbol.  The authors of this book, who have endured too many language standardization brawls, would like to avoid the entire issue.  Once we have introduced quotation in section [[#section-2.3][2.3]], we will denote the empty list as ~'()~ and dispense with the variable ~nil~ entirely.

[fn:77] To define ~f~ and ~g~ using ~lambda~ we would write

#+begin_src scheme
(define f (lambda (x y . z) <BODY>))
(define g (lambda w <BODY>))
#+end_src

[fn:78] Scheme standardly provides a ~map~ procedure that is more general than the one described here.  This more general ~map~ takes a procedure of n arguments, together with n lists, and applies the procedure to all the first elements of the lists, all the second elements of the lists, and so on, returning a list of the results.  For example:

#+begin_src scheme
(map + (list 1 2 3) (list 40 50 60) (list 700 800 900))
(741 852 963)

(map (lambda (x y) (+ x (* 2 y)))
     (list 1 2 3)
     (list 4 5 6))
(9 12 15)
#+end_src

[fn:79] The order of the first two clauses in the ~cond~ matters, since the empty list satisfies ~null?~ and also is not a pair.

[fn:80] This is, in fact, precisely the ~fringe~ procedure from [[#exercise-2.28][Exercise 2.28]].  Here we've renamed it to emphasize that it is part of a family of general sequence-manipulation procedures.

[fn:81] Richard Waters (1979) developed a program that automatically analyzes traditional Fortran programs, viewing them in terms of maps, filters, and accumulations.  He found that fully 90 percent of the code in the Fortran Scientific Subroutine Package fits neatly into this paradigm.  One of the reasons for the success of Lisp as a programming language is that lists provide a standard medium for expressing ordered collections so that they can be manipulated using higher-order operations.  The programming language APL owes much of its power and appeal to a similar choice.  In APL all data are represented as arrays, and there is a universal and convenient set of generic operators for all sorts of array operations.

[fn:82] According to Knuth (1981), this rule was formulated by W. G. Horner early in the nineteenth century, but the method was actually used by Newton over a hundred years earlier.  Horner's rule evaluates the polynomial using fewer additions and multiplications than does the straightforward method of first computing a_n x^n, then adding a_(n-1)x^(n-1), and so on.  In fact, it is possible to prove that any algorithm for evaluating arbitrary polynomials must use at least as many additions and multiplications as does Horner's rule, and thus Horner's rule is an optimal algorithm for polynomial evaluation.  This was proved (for the number of additions) by A. M. Ostrowski in a 1954 paper that essentially founded the modern study of optimal algorithms.  The analogous statement for multiplications was proved by V. Y. Pan in 1966.  The book by Borodin and Munro (1975) provides an overview of these and other results about optimal algorithms.

[fn:83] This definition uses the extended version of ~map~ described in [fn:12].

[fn:84] This approach to nested mappings was shown to us by David Turner, whose languages KRC and Miranda provide elegant formalisms for dealing with these constructs.  The examples in this section (see also [[#exercise-2.42][Exercise 2.42]]) are adapted from Turner 1981.  In section [[#section-3.5.3][3.5.3]], we'll see how this approach generalizes to infinite sequences.

[fn:85] We're representing a pair here as a list of two elements rather than as a Lisp pair.  Thus, the "pair" (i,j) is represented as ~(list i j)~, not ~(cons i j)~.

[fn:86] The set S - x is the set of all elements of S, excluding x.

[fn:87] Semicolons in Scheme code are used to introduce <<i68>> comments.  Everything from the semicolon to the end of the line is ignored by the interpreter.  In this book we don't use many comments; we try to make our programs self-documenting by using descriptive names.

[fn:88] The picture language is based on the language Peter Henderson created to construct images like M.C. Escher's "Square Limit" woodcut (see Henderson 1982).  The woodcut incorporates a repeated scaled pattern, similar to the arrangements drawn using the ~square-limit~ procedure in this section.

[fn:89] William Barton Rogers (1804-1882) was the founder and first president of MIT.  A geologist and talented teacher, he taught at William and Mary College and at the University of Virginia.  In 1859 he moved to Boston, where he had more time for research, worked on a plan for establishing a "polytechnic institute," and served as Massachusetts's first State Inspector of Gas Meters.

When MIT was established in 1861, Rogers was elected its first president.  Rogers espoused an ideal of "useful learning" that was different from the university education of the time, with its overemphasis on the classics, which, as he wrote, "stand in the way of the broader, higher and more practical instruction and discipline of the natural and social sciences."  This education was likewise to be different from narrow trade-school education.  In Rogers's words:

#+begin_quote
The world-enforced distinction between the practical and the scientific worker is utterly futile, and the whole experience of modern times has demonstrated its utter worthlessness.
#+end_quote

Rogers served as president of MIT until 1870, when he resigned due to ill health.  In 1878 the second president of MIT, John Runkle, resigned under the pressure of a financial crisis brought on by the Panic of 1873 and strain of fighting off attempts by Harvard to take over MIT.  Rogers returned to hold the office of president until 1881.

Rogers collapsed and died while addressing MIT's graduating class at the commencement exercises of 1882.  Runkle quoted Rogers's last words in a memorial address delivered that same year:

#+begin_quote
"As I stand here today and see what the Institute is, ... I call to mind the beginnings of science.  I remember one hundred and fifty years ago Stephen Hales published a pamphlet on the subject of illuminating gas, in which he stated that his researches had demonstrated that 128 grains of bituminous coal - " "Bituminous coal," these were his last words on earth.  Here he bent forward, as if consulting some notes on the table before him, then slowly regaining an erect position, threw up his hands, and was translated from the scene of his earthly labors and triumphs to "the tomorrow of death," where the mysteries of life are solved, and the disembodied spirit finds unending satisfaction in contemplating the new and still unfathomable mysteries of the infinite future.
#+end_quote

In the words of Francis A. Walker (MIT's third president):

#+begin_quote
All his life he had borne himself most faithfully and heroically, and he died as so good a knight would surely have wished, in harness, at his post, and in the very part and act of public duty.
#+end_quote

[fn:90] Equivalently, we could write

#+begin_src scheme
(define flipped-pairs
  (square-of-four identity flip-vert identity flip-vert))
#+end_src

[fn:91] ~Rotate180~ rotates a painter by 180 degrees (see [[#exercise-2.50][Exercise 2.50]]).  Instead of ~rotate180~ we could say ~(compose flip-vert flip-horiz)~, using the ~compose~ procedure from [[#exercise-1.42][Exercise 1.42]].

[fn:92] ~Frame-coord-map~ uses the vector operations described in [[#exercise-2.46][Exercise 2.46]] below, which we assume have been implemented using some representation for vectors.  Because of data abstraction, it doesn't matter what this vector representation is, so long as the vector operations behave correctly.

[fn:93] ~Segments->painter~ uses the representation for line segments described in [[#exercise-2.48][Exercise 2.48]] below.  It also uses the ~for-each~ procedure described in [[#exercise-2.23][Exercise 2.23]].

[fn:94] For example, the ~rogers~ painter of [[figure-2.11][Figure 2.11]] was constructed from a gray-level image.  For each point in a given frame, the ~rogers~ painter determines the point in the image that is mapped to it under the frame coordinate map, and shades it accordingly.  By allowing different types of painters, we are capitalizing on the abstract data idea discussed in section [[#section-2.1.3][2.1.3]], where we argued that a rational-number representation could be anything at all that satisfies an appropriate condition.  Here we're using the fact that a painter can be implemented in any way at all, so long as it draws something in the designated frame.  Section [[#section-2.1.3][2.1.3]] also showed how pairs could be implemented as procedures.  Painters are our second example of a procedural representation for data.

[fn:95] ~Rotate90~ is a pure rotation only for square frames, because it also stretches and shrinks the image to fit into the rotated frame.

[fn:96] The diamond-shaped images in figures [[figure-2.10][Figure 2.10]] and [[figure-2.11][Figure 2.11]] were created with ~squash-inwards~ applied to ~wave~ and ~rogers~.

[fn:97] Section [[#section-3.3.4][3.3.4]] describes one such language.

[fn:98] Allowing quotation in a language wreaks havoc with the ability to reason about the language in simple terms, because it destroys the notion that equals can be substituted for equals.  For example, three is one plus two, but the word "three" is not the phrase "one plus two."  Quotation is powerful because it gives us a way to build expressions that manipulate other expressions (as we will see when we write an interpreter in [[#section-4][Chapter 4]]).  But allowing statements in a language that talk about other statements in that language makes it very difficult to maintain any coherent principle of what "equals can be substituted for equals" should mean.  For example, if we know that the evening star is the morning star, then from the statement "the evening star is Venus" we can deduce "the morning star is Venus."  However, given that "John knows that the evening star is Venus" we cannot infer that "John knows that the morning star is Venus."

[fn:99] The single quote is different from the double quote we have been using to enclose character strings to be printed.  Whereas the single quote can be used to denote lists or symbols, the double quote is used only with character strings.  In this book, the only use for character strings is as items to be printed.
