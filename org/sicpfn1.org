[fn:100] Strictly, our use of the quotation mark violates the general rule that all compound expressions in our language should be delimited by parentheses and look like lists.  We can recover this consistency by introducing a special form ~quote~, which serves the same purpose as the quotation mark.  Thus, we would type ~(quote a)~ instead of ~'a~, and we would type ~(quote (a b c))~ instead of ~'(a b c)~.  This is precisely how the interpreter works.  The quotation mark is just a single-character abbreviation for wrapping the next complete expression with ~quote~ to form '(quote <EXPRESSION>)'.  This is important because it maintains the principle that any expression seen by the interpreter can be manipulated as a data object.  For instance, we could construct the expression ~(car '(a b c))~, which is the same as ~(car (quote (a b c)))~, by evaluating ~(list 'car (list 'quote '(a b c)))~.

[fn:101] We can consider two symbols to be "the same" if they consist of the same characters in the same order.  Such a definition skirts a deep issue that we are not yet ready to address: the meaning of "sameness" in a programming language.  We will return to this in [[#section-3][Chapter 3]] (section [[#section-3.1.3][3.1.3]]).

[fn:102] In practice, programmers use ~equal?~ to compare lists that contain numbers as well as symbols.  Numbers are not considered to be symbols.  The question of whether two numerically equal numbers (as tested by ~=~) are also ~eq?~ is highly implementation-dependent.  A better definition of ~equal?~ (such as the one that comes as a primitive in Scheme) would also stipulate that if ~a~ and ~b~ are both numbers, then ~a~ and ~b~ are ~equal?~ if they are numerically equal.

[fn:103] If we want to be more formal, we can specify "consistent with the interpretations given above" to mean that the operations satisfy a collection of rules such as these:

- For any set ~S~ and any object ~x~, ~(element-of-set?  x (adjoin-set x S))~ is true (informally: "Adjoining an object to a set produces a set that contains the object").

- For any sets ~S~ and ~T~ and any object ~x~, ~(element-of-set?  x (union-set S T))~ is equal to ~(or (element-of-set?  x S) (element-of-set?  x T))~ (informally: "The elements of ~(union S T)~ are the elements that are in ~S~ or in ~T~").

- For any object ~x~, ~(element-of-set?  x '())~ is false (informally: "No object is an element of the empty set").

[fn:104] Halving the size of the problem at each step is the distinguishing characteristic of logarithmic growth, as we saw with the fast-exponentiation algorithm of section [[#section-1.2.4][1.2.4]] and the half-interval search method of section [[#section-1.3.3][1.3.3]].

[fn:105] We are representing sets in terms of trees, and trees in terms of lists--in effect, a data abstraction built upon a data abstraction.  We can regard the procedures ~entry~, ~left-branch~, ~right-branch~, and ~make-tree~ as a way of isolating the abstraction of a "binary tree" from the particular way we might wish to represent such a tree in terms of list structure.

[fn:106] Examples of such structures include <<i28>> B-trees and <<i328>> red-black trees.  There is a large literature on data structures devoted to this problem.  See Cormen, Leiserson, and Rivest 1990.

[fn:107] [[#exercise-2.63][Exercise 2.63]] through [[#exercise-2.65][Exercise 2.65]] are due to Paul Hilfinger.

[fn:108] See Hamming 1980 for a discussion of the mathematical properties of Huffman codes.

[fn:109] In actual computational systems, rectangular form is preferable to polar form most of the time because of roundoff errors in conversion between rectangular and polar form.  This is why the complex-number example is unrealistic.  Nevertheless, it provides a clear illustration of the design of a system using generic operations and a good introduction to the more substantial systems to be developed later in this chapter.

[fn:110] The arctangent function referred to here, computed by Scheme's ~atan~ procedure, is defined so as to take two arguments y and x and to return the angle whose tangent is y/x.  The signs of the arguments determine the quadrant of the angle.

[fn:111] We use the list ~(rectangular)~ rather than the symbol ~rectangular~ to allow for the possibility of operations with multiple arguments, not all of the same type.

[fn:112] The type the constructors are installed under needn't be a list because a constructor is always used to make an object of one particular type.

[fn:113] ~Apply-generic~ uses the dotted-tail notation described in [[#exercise-2.20][Exercise 2.20]], because different generic operations may take different numbers of arguments.  In ~apply-generic~, ~op~ has as its value the first argument to ~apply-generic~ and ~args~ has as its value a list of the remaining arguments.

~apply-generic~ also uses the primitive procedure ~apply~, which takes two arguments, a procedure and a list.  ~apply~ applies the procedure, using the elements in the list as arguments.  For example,

#+begin_src scheme
(apply + (list 1 2 3 4))
#+end_src

returns 10.

[fn:114] One limitation of this organization is it permits only generic procedures of one argument.

[fn:115] We also have to supply an almost identical procedure to handle the types ~(scheme-number complex)~.

[fn:116] See [[#exercise-2.82][Exercise 2.82]] for generalizations.

[fn:117] If we are clever, we can usually get by with fewer than n^2 coercion procedures.  For instance, if we know how to convert from type 1 to type 2 and from type 2 to type 3, then we can use this knowledge to convert from type 1 to type 3.  This can greatly decrease the number of coercion procedures we need to supply explicitly when we add a new type to the system.  If we are willing to build the required amount of sophistication into our system, we can have it search the "graph" of relations among types and automatically generate those coercion procedures that can be inferred from the ones that are supplied explicitly.

[fn:118] This statement, which also appears in the first edition of this book, is just as true now as it was when we wrote it twelve years ago.  Developing a useful, general framework for expressing the relations among different types of entities (what philosophers call "ontology") seems intractably difficult.  The main difference between the confusion that existed ten years ago and the confusion that exists now is that now a variety of inadequate ontological theories have been embodied in a plethora of correspondingly inadequate programming languages.  For example, much of the complexity of object-oriented programming languages--and the subtle and confusing differences among contemporary object-oriented languages--centers on the treatment of generic operations on interrelated types.  Our own discussion of computational objects in [[#section-3][Chapter 3]] avoids these issues entirely.  Readers familiar with object-oriented programming will notice that we have much to say in [[#section-3][Chapter 3]] about local state, but we do not even mention "classes" or "inheritance."  In fact, we suspect that these problems cannot be adequately addressed in terms of computer-language design alone, without also drawing on work in knowledge representation and automated reasoning.

[fn:119] A real number can be projected to an integer using the ~round~ primitive, which returns the closest integer to its argument.

[fn:120] On the other hand, we will allow polynomials whose coefficients are themselves polynomials in other variables.  This will give us essentially the same representational power as a full multivariate system, although it does lead to coercion problems, as discussed below.

[fn:121] For univariate polynomials, giving the value of a polynomial at a given set of points can be a particularly good representation.  This makes polynomial arithmetic extremely simple.  To obtain, for example, the sum of two polynomials represented in this way, we need only add the values of the polynomials at corresponding points.  To transform back to a more familiar representation, we can use the Lagrange interpolation formula, which shows how to recover the coefficients of a polynomial of degree n given the values of the polynomial at n + 1 points.

[fn:122] This operation is very much like the ordered ~union-set~ operation we developed in exercise [[#exercise-2.62][Exercise 2.62]].  In fact, if we think of the terms of the polynomial as a set ordered according to the power of the indeterminate, then the program that produces the term list for a sum is almost identical to ~union-set~.

[fn:123] To make this work completely smoothly, we should also add to our generic arithmetic system the ability to coerce a "number" to a polynomial by regarding it as a polynomial of degree zero whose coefficient is the number.  This is necessary if we are going to perform operations such as

#+begin_example
 [x^2 + (y + 1)x + 5] + [x^2 + 2x + 1]
#+end_example

which requires adding the coefficient y + 1 to the coefficient 2.

[fn:124] In these polynomial examples, we assume that we have implemented the generic arithmetic system using the type mechanism suggested in [[#exercise-2.78][Exercise 2.78]].  Thus, coefficients that are ordinary numbers will be represented as the numbers themselves rather than as pairs whose ~car~ is the symbol ~scheme-number~.

[fn:125] Although we are assuming that term lists are ordered, we have implemented ~adjoin-term~ to simply ~cons~ the new term onto the existing term list.  We can get away with this so long as we guarantee that the procedures (such as ~add-terms~) that use ~adjoin-term~ always call it with a higher-order term than appears in the list.  If we did not want to make such a guarantee, we could have implemented ~adjoin-term~ to be similar to the ~adjoin-set~ constructor for the ordered-list representation of sets ([[#exercise-2.61][Exercise 2.61]]).

[fn:126] The fact that Euclid's Algorithm works for polynomials is formalized in algebra by saying that polynomials form a kind of algebraic domain called a <<i127>> Euclidean ring.  A Euclidean ring is a domain that admits addition, subtraction, and commutative multiplication, together with a way of assigning to each element x of the ring a positive integer "measure" m(x) with the properties that m(xy)>= m(x) for any nonzero x and y and that, given any x and y, there exists a q such that y = qx + r and either r = 0 or m(r)< m(x).  From an abstract point of view, this is what is needed to prove that Euclid's Algorithm works.  For the domain of integers, the measure m of an integer is the absolute value of the integer itself.  For the domain of polynomials, the measure of a polynomial is its degree.

[fn:127] In an implementation like MIT Scheme, this produces a polynomial that is indeed a divisor of Q_1 and Q_2, but with rational coefficients.  In many other Scheme systems, in which division of integers can produce limited-precision decimal numbers, we may fail to get a valid divisor.

[fn:128] One extremely efficient and elegant method for computing polynomial GCDs was discovered by Richard Zippel (1979).  The method is a probabilistic algorithm, as is the fast test for primality that we discussed in [[#section-1][Chapter 1]].  Zippel's book (1993) describes this method, together with other ways to compute polynomial GCDs.

[fn:129] Actually, this is not quite true.  One exception was the random-number generator in section [[#section-1.2.6][1.2.6]].  Another exception involved the operation/type tables we introduced in section [[#section-2.4.3][2.4.3]], where the values of two calls to ~get~ with the same arguments depended on intervening calls to ~put~.  On the other hand, until we introduce assignment, we have no way to create such procedures ourselves.

[fn:130] The value of a ~set!~ expression is implementation-dependent.  ~set!~ should be used only for its effect, not for its value.

The name ~set!~ reflects a naming convention used in Scheme: Operations that change the values of variables (or that change data structures, as we will see in section [[#section-3.3][3.3]]) are given names that end with an exclamation point.  This is similar to the convention of designating predicates by names that end with a question mark.

[fn:131] We have already used ~begin~ implicitly in our programs, because in Scheme the body of a procedure can be a sequence of expressions.  Also, the <CONSEQUENT> part of each clause in a ~cond~ expression can be a sequence of expressions rather than a single expression.

[fn:132] In programming-language jargon, the variable ~balance~ is said to be <<i119>> encapsulated within the ~new-withdraw~ procedure.  Encapsulation reflects the general system-design principle known as the <<i174>> hiding principle: One can make a system more modular and robust by protecting parts of the system from each other; that is, by providing information access only to those parts of the system that have a "need to know."

[fn:133] In contrast with ~new-withdraw~ above, we do not have to use ~let~ to make ~balance~ a local variable, since formal parameters are already local.  This will be clearer after the discussion of the environment model of evaluation in section [[#section-3.2][3.2]].  (See also [[#exercise-3.10][Exercise 3.10]].)

[fn:134] One common way to implement ~rand-update~ is to use the rule that x is updated to ax + b modulo m, where a, b, and m are appropriately chosen integers.  Chapter 3 of Knuth 1981 includes an extensive discussion of techniques for generating sequences of random numbers and establishing their statistical properties.  Notice that the ~rand-update~ procedure computes a mathematical function: Given the same input twice, it produces the same output.  Therefore, the number sequence produced by ~rand-update~ certainly is not "random," if by "random" we insist that each number in the sequence is unrelated to the preceding number.  The relation between "real randomness" and so-called <<i308>> pseudo-random sequences, which are produced by well-determined computations and yet have suitable statistical properties, is a complex question involving difficult issues in mathematics and philosophy.  Kolmogorov, Solomonoff, and Chaitin have made great progress in clarifying these issues; a discussion can be found in Chaitin 1975.

[fn:135] This theorem is due to E. Cesa`ro.  See section 4.5.2 of Knuth 1981 for a discussion and a proof.

[fn:136] MIT Scheme provides such a procedure.  If ~random~ is given an exact integer (as in section [[#section-1.2.6][1.2.6]]) it returns an exact integer, but if it is given a decimal value (as in this exercise) it returns a decimal value.

[fn:137] We don't substitute for the occurrence of ~balance~ in the ~set!~ expression because the <NAME> in a ~set!~ is not evaluated.  If we did substitute for it, we would get ~(set!  25 (- 25 amount))~, which makes no sense.

[fn:138] The phenomenon of a single computational object being accessed by more than one name is known as <<i16>> aliasing.  The joint bank account situation illustrates a very simple example of an alias.  In section [[#section-3.3][3.3]] we will see much more complex examples, such as "distinct" compound data structures that share parts.  Bugs can occur in our programs if we forget that a change to an object may also, as a "side effect," change a "different" object because the two "different" objects are actually a single object appearing under different aliases.  These so-called <<i354>> side-effect bugs are so difficult to locate and to analyze that some people have proposed that programming languages be designed in such a way as to not allow side effects or aliasing (Lampson et al.  1981; Morris, Schmidt, and Wadler 1980).

[fn:139] In view of this, it is ironic that introductory programming is most often taught in a highly imperative style.  This may be a vestige of a belief, common throughout the 1960s and 1970s, that programs that call procedures must inherently be less efficient than programs that perform assignments.  (Steele (1977) debunks this argument.)  Alternatively it may reflect a view that step-by-step assignment is easier for beginners to visualize than procedure call.  Whatever the reason, it often saddles beginning programmers with "should I set this variable before or after that one" concerns that can complicate programming and obscure the important ideas.

[fn:140] ssignment introduces a subtlety into step 1 of the evaluation rule.  As shown in [[#exercise-3.8][Exercise 3.8]], the presence of assignment allows us to write expressions that will produce different values depending on the order in which the subexpressions in a combination are evaluated.  Thus, to be precise, we should specify an evaluation order in step 1 (e.g., left to right or right to left).  However, this order should always be considered to be an implementation detail, and one should never write programs that depend on some particular order.  For instance, a sophisticated compiler might optimize a program by varying the order in which subexpressions are evaluated.

[fn:141] If there is already a binding for the variable in the current frame, then the binding is changed.  This is convenient because it allows redefinition of symbols; however, it also means that ~define~ can be used to change values, and this brings up the issues of assignment without explicitly using ~set!~.  Because of this, some people prefer redefinitions of existing symbols to signal errors or warnings.

[fn:142] The environment model will not clarify our claim in section [[#section-1.2.1][1.2.1]] that the interpreter can execute a procedure such as ~fact-iter~ in a constant amount of space using tail recursion.  We will discuss tail recursion when we deal with the control structure of the interpreter in section [[#section-5.4][5.4]].

[fn:143] Whether ~W1~ and ~W2~ share the same physical code stored in the computer, or whether they each keep a copy of the code, is a detail of the implementation.  For the interpreter we implement in [[#section-4][Chapter 4]], the code is in fact shared.

[fn:144] ~Set-car!~ and ~set-cdr!~ return implementation-dependent values.  Like ~set!~, they should be used only for their effect.

[fn:145] We see from this that mutation operations on lists can create "garbage" that is not part of any accessible structure.  We will see in section [[#section-5.3.2][5.3.2]] that Lisp memory-management systems include a <<i159>> garbage collector, which identifies and recycles the memory space used by unneeded pairs.

[fn:146] ~Get-new-pair~ is one of the operations that must be implemented as part of the memory management required by a Lisp implementation.  We will discuss this in section [[#section-5.3.1][5.3.1]].

[fn:147] The two pairs are distinct because each call to ~cons~ returns a new pair.  The symbols are shared; in Scheme there is a unique symbol with any given name.  Since Scheme provides no way to mutate a symbol, this sharing is undetectable.  Note also that the sharing is what enables us to compare symbols using ~eq?~, which simply checks equality of pointers.

[fn:148] The subtleties of dealing with sharing of mutable data objects reflect the underlying issues of "sameness" and "change" that were raised in section [[#section-3.1.3][3.1.3]].  We mentioned there that admitting change to our language requires that a compound object must have an "identity" that is something different from the pieces from which it is composed.  In Lisp, we consider this "identity" to be the quality that is tested by ~eq?~, i.e., by equality of pointers.  Since in most Lisp implementations a pointer is essentially a memory address, we are "solving the problem" of defining the identity of objects by stipulating that a data object "itself" is the information stored in some particular set of memory locations in the computer.  This suffices for simple Lisp programs, but is hardly a general way to resolve the issue of "sameness" in computational models.

[fn:149] On the other hand, from the viewpoint of implementation, assignment requires us to modify the environment, which is itself a mutable data structure.  Thus, assignment and mutation are equipotent: Each can be implemented in terms of the other.

[fn:150] If the first item is the final item in the queue, the front pointer will be the empty list after the deletion, which will mark the queue as empty; we needn't worry about updating the rear pointer, which will still point to the deleted item, because ~empty-queue?~ looks only at the front pointer.

[fn:151] Be careful not to make the interpreter try to print a structure that contains cycles.  (See [[#exercise-3.13][Exercise 3.13]].)

[fn:152] Because ~assoc~ uses ~equal?~, it can recognize keys that are symbols, numbers, or list structure.

[fn:153] Thus, the first backbone pair is the object that represents the table "itself"; that is, a pointer to the table is a pointer to this pair.  This same backbone pair always starts the table.  If we did not arrange things in this way, ~insert!~ would have to return a new value for the start of the table when it added a new record.

[fn:154] A full-adder is a basic circuit element used in adding two binary numbers.  Here A and B are the bits at corresponding positions in the two numbers to be added, and C_(in) is the carry bit from the addition one place to the right.  The circuit generates SUM, which is the sum bit in the corresponding position, and C_(out), which is the carry bit to be propagated to the left.

[fn:155] These procedures are simply syntactic sugar that allow us to use ordinary procedural syntax to access the local procedures of objects.  It is striking that we can interchange the role of "procedures" and "data" in such a simple way.  For example, if we write ~(wire 'get-signal)~ we think of ~wire~ as a procedure that is called with the message ~get-signal~ as input.  Alternatively, writing ~(get-signal wire)~ encourages us to think of ~wire~ as a data object that is the input to a procedure ~get-signal~.  The truth of the matter is that, in a language in which we can deal with procedures as objects, there is no fundamental difference between "procedures" and "data," and we can choose our syntactic sugar to allow us to program in whatever style we choose.

[fn:156] The agenda is a headed list, like the tables in section [[#section-3.3.3][3.3.3]], but since the list is headed by the time, we do not need an additional dummy header (such as the ~*table*~ symbol used with tables).

[fn:157] Observe that the ~if~ expression in this procedure has no <ALTERNATIVE> expression.  Such a "one-armed ~if~ statement" is used to decide whether to do something, rather than to select between two expressions.  An ~if~ expression returns an unspecified value if the predicate is false and there is no <ALTERNATIVE>.

[fn:158] In this way, the current time will always be the time of the action most recently processed.  Storing this time at the head of the agenda ensures that it will still be available even if the associated time segment has been deleted.

[fn:159] Constraint propagation first appeared in the incredibly forward-looking SKETCHPAD system of Ivan Sutherland (1963).  A beautiful constraint-propagation system based on the Smalltalk language was developed by Alan Borning (1977) at Xerox Palo Alto Research Center.  Sussman, Stallman, and Steele applied constraint propagation to electrical circuit analysis (Sussman and Stallman 1975; Sussman and Steele 1980).  TK!Solver (Konopasek and Jayaraman 1984) is an extensive modeling environment based on constraints.

[fn:160] The ~setter~ might not be a constraint.  In our temperature example, we used ~user~ as the ~setter~.

[fn:161] The expression-oriented format is convenient because it avoids the need to name the intermediate expressions in a computation.  Our original formulation of the constraint language is cumbersome in the same way that many languages are cumbersome when dealing with operations on compound data.  For example, if we wanted to compute the product (a + b) * (c + d), where the variables represent vectors, we could work in "imperative style," using procedures that set the values of designated vector arguments but do not themselves return vectors as values:

#+begin_src scheme
(v-sum a b temp1)
(v-sum c d temp2)
(v-prod temp1 temp2 answer)
#+end_src

Alternatively, we could deal with expressions, using procedures that return vectors as values, and thus avoid explicitly mentioning ~temp1~ and ~temp2~:

#+begin_src scheme
(define answer (v-prod (v-sum a b) (v-sum c d)))
#+end_src

Since Lisp allows us to return compound objects as values of procedures, we can transform our imperative-style constraint language into an expression-oriented style as shown in this exercise.  In languages that are impoverished in handling compound objects, such as Algol, Basic, and Pascal (unless one explicitly uses Pascal pointer variables), one is usually stuck with the imperative style when manipulating compound objects.  Given the advantage of the expression-oriented format, one might ask if there is any reason to have implemented the system in imperative style, as we did in this section.  One reason is that the non-expression-oriented constraint language provides a handle on constraint objects (e.g., the value of the ~adder~ procedure) as well as on connector objects.  This is useful if we wish to extend the system with new operations that communicate with constraints directly rather than only indirectly via operations on connectors.  Although it is easy to implement the expression-oriented style in terms of the imperative implementation, it is very difficult to do the converse.

[fn:162] Most real processors actually execute a few operations at a time, following a strategy called <<i286>> pipelining.  Although this technique greatly improves the effective utilization of the hardware, it is used only to speed up the execution of a sequential instruction stream, while retaining the behavior of the sequential program.

[fn:163] To quote some graffiti seen on a Cambridge building wall: "Time is a device that was invented to keep everything from happening at once."

[fn:164] An even worse failure for this system could occur if the two ~set!~ operations attempt to change the balance simultaneously, in which case the actual data appearing in memory might end up being a random combination of the information being written by the two processes.  Most computers have interlocks on the primitive memory-write operations, which protect against such simultaneous access.  Even this seemingly simple kind of protection, however, raises implementation challenges in the design of multiprocessing computers, where elaborate <<i46>> cache-coherence protocols are required to ensure that the various processors will maintain a consistent view of memory contents, despite the fact that data may be replicated ("cached") among the different processors to increase the speed of memory access.

[fn:165] The factorial program in section [[#section-3.1.3][3.1.3]] illustrates this for a single sequential process.

[fn:166] The columns show the contents of Peter's wallet, the joint account (in Bank1), Paul's wallet, and Paul's private account (in Bank2), before and after each withdrawal (W) and deposit (D). Peter withdraws $10 from Bank1; Paul deposits $5 in Bank2, then withdraws $25 from Bank1.

[fn:167] A more formal way to express this idea is to say that concurrent programs are inherently <<i258>> nondeterministic.  That is, they are described not by single-valued functions, but by functions whose results are sets of possible values.  In section [[#section-4.3][4.3]] we will study a language for expressing nondeterministic computations.

[fn:168] ~Parallel-execute~ is not part of standard Scheme, but it can be implemented in MIT Scheme.  In our implementation, the new concurrent processes also run concurrently with the original Scheme process.  Also, in our implementation, the value returned by ~parallel-execute~ is a special control object that can be used to halt the newly created processes.

[fn:169] We have simplified ~exchange~ by exploiting the fact that our ~deposit~ message accepts negative amounts.  (This is a serious bug in our banking system!)

[fn:170] If the account balances start out as $10, $20, and $30, then after any number of concurrent exchanges, the balances should still be $10, $20, and $30 in some order.  Serializing the deposits to individual accounts is not sufficient to guarantee this.  See [[#exercise-3.43][Exercise 3.43]].

[fn:171] [[#exercise-3.45][Exercise 3.45]] investigates why deposits and withdrawals are no longer automatically serialized by the account.

[fn:172] The term "mutex" is an abbreviation for <<i249>> mutual exclusion.  The general problem of arranging a mechanism that permits concurrent processes to safely share resources is called the mutual exclusion problem.  Our mutex is a simple variant of the <<i344>> semaphore mechanism (see [[#exercise-3.47][Exercise 3.47]]), which was introduced in the "THE" Multiprogramming System developed at the Technological University of Eindhoven and named for the university's initials in Dutch (Dijkstra 1968a).  The acquire and release operations were originally called P and V, from the Dutch words /passeren/ (to pass) and /vrijgeven/ (to release), in reference to the semaphores used on railroad systems.  Dijkstra's classic exposition (1968b) was one of the first to clearly present the issues of concurrency control, and showed how to use semaphores to handle a variety of concurrency problems.

[fn:173] In most time-shared operating systems, processes that are blocked by a mutex do not waste time "busy-waiting" as above.  Instead, the system schedules another process to run while the first is waiting, and the blocked process is awakened when the mutex becomes available.

[fn:174] In MIT Scheme for a single processor, which uses a time-slicing model, ~test-and-set!~ can be implemented as follows:

#+begin_src scheme
(define (test-and-set! cell)
  (without-interrupts
   (lambda ()
     (if (car cell)
         true
         (begin (set-car! cell true)
                false)))))
#+end_src

~without-interrupts~ disables time-slicing interrupts while its procedure argument is being executed.

[fn:175] There are many variants of such instructions--including test-and-set, test-and-clear, swap, compare-and-exchange, load-reserve, and store-conditional--whose design must be carefully matched to the machine's processor-memory interface.  One issue that arises here is to determine what happens if two processes attempt to acquire the same resource at exactly the same time by using such an instruction.  This requires some mechanism for making a decision about which process gets control.  Such a mechanism is called an <<i20>> arbiter.  Arbiters usually boil down to some sort of hardware device.  Unfortunately, it is possible to prove that one cannot physically construct a fair arbiter that works 100% of the time unless one allows the arbiter an arbitrarily long time to make its decision.  The fundamental phenomenon here was originally observed by the fourteenth-century French philosopher Jean Buridan in his commentary on Aristotle's De caelo.  Buridan argued that a perfectly rational dog placed between two equally attractive sources of food will starve to death, because it is incapable of deciding which to go to first.

[fn:176] The general technique for avoiding deadlock by numbering the shared resources and acquiring them in order is due to Havender (1968).  Situations where deadlock cannot be avoided require <<i100>> deadlock-recovery methods, which entail having processes "back out" of the deadlocked state and try again.  Deadlock-recovery mechanisms are widely used in database management systems, a topic that is treated in detail in Gray and Reuter 1993.

[fn:177] One such alternative to serialization is called <<i33>> barrier synchronization.  The programmer permits concurrent processes to execute as they please, but establishes certain synchronization points ("barriers") through which no process can proceed until all the processes have reached the barrier.  Modern processors provide machine instructions that permit programmers to establish synchronization points at places where consistency is required.  The PowerPC^( TM), for example, includes for this purpose two instructions called SYNC and EIEIO (Enforced In-order Execution of Input/Output).

[fn:178] This may seem like a strange point of view, but there are systems that work this way.  International charges to credit-card accounts, for example, are normally cleared on a per-country basis, and the charges made in different countries are periodically reconciled.  Thus the account balance may be different in different countries.

[fn:179] For distributed systems, this perspective was pursued by Lamport (1978), who showed how to use communication to establish "global clocks" that can be used to establish orderings on events in distributed systems.

[fn:180] Physicists sometimes adopt this view by introducing the "world lines" of particles as a device for reasoning about motion.  We've also already mentioned (section [[#section-2.2.3][2.2.3]]) that this is the natural way to think about signal-processing systems.  We will explore applications of streams to signal processing in section [[#section-3.5.3][3.5.3]].

[fn:181] Assume that we have a predicate ~prime?~ (e.g., as in section [[#section-1.2.6][1.2.6]]) that tests for primality.

[fn:182] In the MIT implementation, ~the-empty-stream~ is the same as the empty list ~'()~, and ~stream-null?~ is the same as ~null?~.

[fn:183] This should bother you.  The fact that we are defining such similar procedures for streams and lists indicates that we are missing some underlying abstraction.  Unfortunately, in order to exploit this abstraction, we will need to exert finer control over the process of evaluation than we can at present.  We will discuss this point further at the end of section [[#section-3.5.4][3.5.4]].  In section [[#section-4.2][4.2]], we'll develop a framework that unifies lists and streams.

[fn:184] Although ~stream-car~ and ~stream-cdr~ can be defined as procedures, ~cons-stream~ must be a special form.  If ~cons-stream~ were a procedure, then, according to our model of evaluation, evaluating '(cons-stream <A> <B>)' would automatically cause <B> to be evaluated, which is precisely what we do not want to happen.  For the same reason, ~delay~ must be a special form, though ~force~ can be an ordinary procedure.

[fn:185] The numbers shown here do not really appear in the delayed expression.  What actually appears is the original expression, in an environment in which the variables are bound to the appropriate numbers.  For example, ~(+ low 1)~ with ~low~ bound to 10,000 actually appears where ~10001~ is shown.

[fn:186] There are many possible implementations of streams other than the one described in this section.  Delayed evaluation, which is the key to making streams practical, was inherent in Algol 60's <<i47>> call-by-name parameter-passing method.  The use of this mechanism to implement streams was first described by Landin (1965).  Delayed evaluation for streams was introduced into Lisp by Friedman and Wise (1976).  In their implementation, ~cons~ always delays evaluating its arguments, so that lists automatically behave as streams.  The memoizing optimization is also known as <<i50>> call-by-need.  The Algol community would refer to our original delayed objects as <<i49>> call-by-name thunks and to the optimized versions as <<i52>> call-by-need thunks.

[fn:187] Exercises such as [[#exercise-3.51][Exercise 3.51]] and [[#exercise-3.52][Exercise 3.52]] are valuable for testing our understanding of how ~delay~ works.  On the other hand, intermixing delayed evaluation with printing--and, even worse, with assignment--is extremely confusing, and instructors of courses on computer languages have traditionally tormented their students with examination questions such as the ones in this section.  Needless to say, writing programs that depend on such subtleties is odious programming style.  Part of the power of stream processing is that it lets us ignore the order in which events actually happen in our programs.  Unfortunately, this is precisely what we cannot afford to do in the presence of assignment, which forces us to be concerned with time and change.

[fn:188] Eratosthenes, a third-century B.C. Alexandrian Greek philosopher, is famous for giving the first accurate estimate of the circumference of the Earth, which he computed by observing shadows cast at noon on the day of the summer solstice.  Eratosthenes's sieve method, although ancient, has formed the basis for special-purpose hardware "sieves" that, until recently, were the most powerful tools in existence for locating large primes.  Since the 70s, however, these methods have been superseded by outgrowths of the probabilistic techniques discussed in section [[#section-1.2.6][1.2.6]].

[fn:189] We have named these figures after Peter Henderson, who was the first person to show us diagrams of this sort as a way of thinking about stream processing.  Each solid line represents a stream of values being transmitted.  The dashed line from the ~car~ to the ~cons~ and the ~filter~ indicates that this is a single value rather than a stream.

[fn:190] This uses the generalized version of ~stream-map~ from [[#exercise-3.50][Exercise 3.50]].

[fn:191] This last point is very subtle and relies on the fact that p_(n+1) <= p_n^2.  (Here, p_k denotes the kth prime.)  Estimates such as these are very difficult to establish.  The ancient proof by Euclid that there are an infinite number of primes shows that p_(n+1)<= p_1 p_2...p_n + 1, and no substantially better result was proved until 1851, when the Russian mathematician P. L.  Chebyshev established that p_(n+1)<= 2p_n for all n.  This result, originally conjectured in 1845, is known as <<i35>> Bertrand's hypothesis.  A proof can be found in section 22.3 of Hardy and Wright 1960.

[fn:192] This exercise shows how call-by-need is closely related to ordinary memoization as described in [[#exercise-3.27][Exercise 3.27]].  In that exercise, we used assignment to explicitly construct a local table.  Our call-by-need stream optimization effectively constructs such a table automatically, storing values in the previously forced parts of the stream.

[fn:193] We can't use ~let~ to bind the local variable ~guesses~, because the value of ~guesses~ depends on ~guesses~ itself.  [[#exercise-3.63][Exercise 3.63]] addresses why we want a local variable here.

[fn:194] As in section [[#section-2.2.3][2.2.3]], we represent a pair of integers as a list rather than a Lisp pair.

[fn:195] See [[#exercise-3.68][Exercise 3.68]] for some insight into why we chose this decomposition.

[fn:196] The precise statement of the required property on the order of combination is as follows: There should be a function f of two arguments such that the pair corresponding to element i of the first stream and element j of the second stream will appear as element number f(i,j) of the output stream.  The trick of using ~interleave~ to accomplish this was shown to us by David Turner, who employed it in the language KRC (Turner 1981).

[fn:197] We will require that the weighting function be such that the weight of a pair increases as we move out along a row or down along a column of the array of pairs.

[fn:198] To quote from G. H. Hardy's obituary of Ramanujan (Hardy 1921): "It was Mr.  Littlewood (I believe) who remarked that 'every positive integer was one of his friends.'  I remember once going to see him when he was lying ill at Putney.  I had ridden in taxi-cab No.  1729, and remarked that the number seemed to me a rather dull one, and that I hoped it was not an unfavorable omen.  'No,' he replied, 'it is a very interesting number; it is the smallest number expressible as the sum of two cubes in two different ways.'  " The trick of using weighted pairs to generate the Ramanujan numbers was shown to us by Charles Leiserson.

[fn:199] This procedure is not guaranteed to work in all Scheme implementations, although for any implementation there is a simple variation that will work.  The problem has to do with subtle differences in the ways that Scheme implementations handle internal definitions.  (See section [[#section-4.1.6][4.1.6]].)
