[fn:300] This idea was invented and first implemented by Minsky, as part of the implementation of Lisp for the PDP-1 at the MIT Research Laboratory of Electronics.  It was further developed by Fenichel and Yochelson (1969) for use in the Lisp implementation for the Multics time-sharing system.  Later, Baker (1978) developed a "real-time" version of the method, which does not require the computation to stop during garbage collection.  Baker's idea was extended by Hewitt, Lieberman, and Moon (see Lieberman and Hewitt 1983) to take advantage of the fact that some structure is more volatile and other structure is more permanent.

An alternative commonly used garbage-collection technique is the <<i226>> mark-sweep method.  This consists of tracing all the structure accessible from the machine registers and marking each pair we reach.  We then scan all of memory, and any location that is unmarked is "swept up" as garbage and made available for reuse.  A full discussion of the mark-sweep method can be found in Allen 1978.

The Minsky-Fenichel-Yochelson algorithm is the dominant algorithm in use for large-memory systems because it examines only the useful part of memory.  This is in contrast to mark-sweep, in which the sweep phase must check all of memory.  A second advantage of stop-and-copy is that it is a <<i69>> compacting garbage collector.  That is, at the end of the garbage-collection phase the useful data will have been moved to consecutive memory locations, with all garbage pairs compressed out.  This can be an extremely important performance consideration in machines with virtual memory, in which accesses to widely separated memory addresses may require extra paging operations.

[fn:301] This list of registers does not include the registers used by the storage-allocation system--~root~, ~the-cars~, ~the-cdrs~, and the other registers that will be introduced in this section.

[fn:302] The term /broken heart/ was coined by David Cressey, who wrote a garbage collector for MDL, a dialect of Lisp developed at MIT during the early 1970s.

[fn:303] The garbage collector uses the low-level predicate ~pointer-to-pair?~ instead of the list-structure ~pair?~ operation because in a real system there might be various things that are treated as pairs for garbage-collection purposes.  For example, in a Scheme system that conforms to the IEEE standard a procedure object may be implemented as a special kind of "pair" that doesn't satisfy the ~pair?~ predicate.  For simulation purposes, ~pointer-to-pair?~ can be implemented as ~pair?~.

[fn:304] See Batali et al.  1982 for more information on the chip and the method by which it was designed.

[fn:305] In our controller, the dispatch is written as a sequence of ~test~ and ~branch~ instructions.  Alternatively, it could have been written in a data-directed style (and in a real system it probably would have been) to avoid the need to perform sequential tests and to facilitate the definition of new expression types.  A machine designed to run Lisp would probably include a ~dispatch-on-type~ instruction that would efficiently execute such data-directed dispatches.

[fn:306] This is an important but subtle point in translating algorithms from a procedural language, such as Lisp, to a register-machine language.  As an alternative to saving only what is needed, we could save all the registers (except ~val~) before each recursive call.  This is called a <<i147>> framed-stack discipline.  This would work but might save more registers than necessary; this could be an important consideration in a system where stack operations are expensive.  Saving registers whose contents will not be needed later may also hold onto useless data that could otherwise be garbage-collected, freeing space to be reused.

[fn:307] We add to the evaluator data-structure procedures in section [[#section-4.1.3][4.1.3]] the following two procedures for manipulating argument lists:

#+begin_src scheme
(define (empty-arglist) '())

(define (adjoin-arg arg arglist)
  (append arglist (list arg)))
#+end_src

We also use an additional syntax procedure to test for the last operand in a combination:

#+begin_src scheme
(define (last-operand? ops)
  (null? (cdr ops)))
#+end_src

[fn:308] The optimization of treating the last operand specially is known as <<i131>> evlis tail recursion (see Wand 1980).  We could be somewhat more efficient in the argument evaluation loop if we made evaluation of the first operand a special case too.  This would permit us to postpone initializing ~argl~ until after evaluating the first operand, so as to avoid saving ~argl~ in this case.  The compiler in section [[#section-5.5][5.5]] performs this optimization.  (Compare the ~construct-arglist~ procedure of section [[#section-5.5.3][5.5.3]].)

[fn:309] The order of operand evaluation in the metacircular evaluator is determined by the order of evaluation of the arguments to ~cons~ in the procedure ~list-of-values~ of section [[#section-4.1.1][4.1.1]] (see [[#exercise-4.1][Exercise 4.1]]).

[fn:310] We saw in section [[#section-5.1][5.1]] how to implement such a process with a register machine that had no stack; the state of the process was stored in a fixed set of registers.

[fn:311] This implementation of tail recursion in ~ev-sequence~ is one variety of a well-known optimization technique used by many compilers.  In compiling a procedure that ends with a procedure call, one can replace the call by a jump to the called procedure's entry point.  Building this strategy into the interpreter, as we have done in this section, provides the optimization uniformly throughout the language.

[fn:312] We can define ~no-more-exps?~ as follows:

#+begin_src scheme
(define (no-more-exps? seq) (null? seq))
#+end_src

[fn:313] This isn't really cheating.  In an actual implementation built from scratch, we would use our explicit-control evaluator to interpret a Scheme program that performs source-level transformations like ~cond->if~ in a syntax phase that runs before execution.

[fn:314] We assume here that ~read~ and the various printing operations are available as primitive machine operations, which is useful for our simulation, but completely unrealistic in practice.  These are actually extremely complex operations.  In practice, they would be implemented using low-level input-output operations such as transferring single characters to and from a device.

To support the ~get-global-environment~ operation we define

#+begin_src scheme
(define the-global-environment (setup-environment))

(define (get-global-environment)
  the-global-environment)
#+end_src

[fn:315] There are other errors that we would like the interpreter to handle, but these are not so simple.  See [[#exercise-5.30][Exercise 5.30]].

[fn:316] We could perform the stack initialization only after errors, but doing it in the driver loop will be convenient for monitoring the evaluator's performance, as described below.

[fn:317] Regrettably, this is the normal state of affairs in conventional compiler-based language systems such as C. In UNIX(tm) the system "dumps core," and in DOS/Windows(tm) it becomes catatonic.  The Macintosh(tm) displays a picture of an exploding bomb and offers you the opportunity to reboot the computer--if you're lucky.

[fn:318] This is a theoretical statement.  We are not claiming that the evaluator's data paths are a particularly convenient or efficient set of data paths for a general-purpose computer.  For example, they are not very good for implementing high-performance floating-point calculations or calculations that intensively manipulate bit vectors.

[fn:319] Actually, the machine that runs compiled code can be simpler than the interpreter machine, because we won't use the ~exp~ and ~unev~ registers.  The interpreter used these to hold pieces of unevaluated expressions.  With the compiler, however, these expressions get built into the compiled code that the register machine will run.  For the same reason, we don't need the machine operations that deal with expression syntax.  But compiled code will use a few additional machine operations (to represent compiled procedure objects) that didn't appear in the explicit-control evaluator machine.

[fn:320] Notice, however, that our compiler is a Scheme program, and the syntax procedures that it uses to manipulate expressions are the actual Scheme procedures used with the metacircular evaluator.  For the explicit-control evaluator, in contrast, we assumed that equivalent syntax operations were available as operations for the register machine.  (Of course, when we simulated the register machine in Scheme, we used the actual Scheme procedures in our register machine simulation.)

[fn:321] This procedure uses a feature of Lisp called <<i30>> backquote (or <<i311>> quasiquote) that is handy for constructing lists.  Preceding a list with a backquote symbol is much like quoting it, except that anything in the list that is flagged with a comma is evaluated.

For example, if the value of ~linkage~ is the symbol ~branch25~, then the expression ~'((goto (label ,linkage)))~ evaluates to the list ~((goto (label branch25)))~.  Similarly, if the value of ~x~ is the list ~(a b c)~, then ~'(1 2 ,(car x))~ evaluates to the list ~(1 2 a)~.

[fn:322] We can't just use the labels ~true-branch~, ~false-branch~, and ~after-if~ as shown above, because there might be more than one ~if~ in the program.  The compiler uses the procedure ~make-label~ to generate labels.  ~make-label~ takes a symbol as argument and returns a new symbol that begins with the given symbol.  For example, successive calls to ~(make-label 'a)~ would return ~a1~, ~a2~, and so on.  ~make-label~ can be implemented similarly to the generation of unique variable names in the query language, as follows:

#+begin_src scheme
(define label-counter 0)

(define (new-label-number)
  (set! label-counter (+ 1 label-counter))
  label-counter)

(define (make-label name)
  (string->symbol
   (string-append (symbol->string name)
                  (number->string (new-label-number)))))
#+end_src

[fn:323] We need machine operations to implement a data structure for representing compiled procedures, analogous to the structure for compound procedures described in section [[#section-4.1.3][4.1.3]]:

#+begin_src scheme
(define (make-compiled-procedure entry env)
  (list 'compiled-procedure entry env))

(define (compiled-procedure? proc)
  (tagged-list? proc 'compiled-procedure))

(define (compiled-procedure-entry c-proc) (cadr c-proc))

(define (compiled-procedure-env c-proc) (caddr c-proc))
#+end_src

[fn:324] Actually, we signal an error when the target is not ~val~ and the linkage is ~return~, since the only place we request ~return~ linkages is in compiling procedures, and our convention is that procedures return their values in ~val~.

[fn:325] Making a compiler generate tail-recursive code might seem like a straightforward idea.  But most compilers for common languages, including C and Pascal, do not do this, and therefore these languages cannot represent iterative processes in terms of procedure call alone.  The difficulty with tail recursion in these languages is that their implementations use the stack to store procedure arguments and local variables as well as return addresses.  The Scheme implementations described in this book store arguments and variables in memory to be garbage-collected.  The reason for using the stack for variables and arguments is that it avoids the need for garbage collection in languages that would not otherwise require it, and is generally believed to be more efficient.  Sophisticated Lisp compilers can, in fact, use the stack for arguments without destroying tail recursion.  (See Hanson 1990 for a description.)  There is also some debate about whether stack allocation is actually more efficient than garbage collection in the first place, but the details seem to hinge on fine points of computer architecture.  (See Appel 1987 and Miller and Rozas 1994 for opposing views on this issue.)

[fn:326] The variable ~all-regs~ is bound to the list of names of all the registers:

#+begin_src scheme
(define all-regs '(env proc val argl continue))
#+end_src

[fn:327] Note that ~preserving~ calls ~append~ with three arguments.  Though the definition of ~append~ shown in this book accepts only two arguments, Scheme standardly provides an ~append~ procedure that takes an arbitrary number of arguments.

[fn:328] We have used the same symbol ~+~ here to denote both the source-language procedure and the machine operation.  In general there will not be a one-to-one correspondence between primitives of the source language and primitives of the machine.

[fn:329] Making the primitives into reserved words is in general a bad idea, since a user cannot then rebind these names to different procedures.  Moreover, if we add reserved words to a compiler that is in use, existing programs that define procedures with these names will stop working.  See [[#exercise-5.44][Exercise 5.44]] for ideas on how to avoid this problem.

[fn:330] This is not true if we allow internal definitions, unless we scan them out.  See [[#exercise-5.43][Exercise 5.43]].

[fn:331] This is the modification to variable lookup required if we implement the scanning method to eliminate internal definitions ([[#exercise-5.43][Exercise 5.43]]).  We will need to eliminate these definitions in order for lexical addressing to work.

[fn:332] Lexical addresses cannot be used to access variables in the global environment, because these names can be defined and redefined interactively at any time.  With internal definitions scanned out, as in [[#exercise-5.43][Exercise 5.43]], the only definitions the compiler sees are those at top level, which act on the global environment.  Compilation of a definition does not cause the defined name to be entered in the compile-time environment.

[fn:333] Of course, compiled procedures as well as interpreted procedures are compound (nonprimitive).  For compatibility with the terminology used in the explicit-control evaluator, in this section we will use "compound" to mean interpreted (as opposed to compiled).

[fn:334] Now that the evaluator machine starts with a ~branch~, we must always initialize the ~flag~ register before starting the evaluator machine.  To start the machine at its ordinary read-eval-print loop, we could use

#+begin_src scheme
(define (start-eceval)
  (set! the-global-environment (setup-environment))
  (set-register-contents! eceval 'flag false)
  (start eceval))
#+end_src

[fn:335] Since a compiled procedure is an object that the system may try to print, we also modify the system print operation ~user-print~ (from section [[#section-4.1.4][4.1.4]]) so that it will not attempt to print the components of a compiled procedure:

#+begin_src scheme
(define (user-print object)
  (cond ((compound-procedure? object)
         (display (list 'compound-procedure
                        (procedure-parameters object)
                        (procedure-body object)
                        '<procedure-env>)))
        ((compiled-procedure? object)
         (display '<compiled-procedure>))
        (else (display object))))
#+end_src

[fn:336] We can do even better by extending the compiler to allow compiled code to call interpreted procedures.  See [[#exercise-5.47][Exercise 5.47]].

[fn:337] Independent of the strategy of execution, we incur significant overhead if we insist that errors encountered in execution of a user program be detected and signaled, rather than being allowed to kill the system or produce wrong answers.  For example, an out-of-bounds array reference can be detected by checking the validity of the reference before performing it.  The overhead of checking, however, can be many times the cost of the array reference itself, and a programmer should weigh speed against safety in determining whether such a check is desirable.  A good compiler should be able to produce code with such checks, should avoid redundant checks, and should allow programmers to control the extent and type of error checking in the compiled code.

Compilers for popular languages, such as C and C++, put hardly any error-checking operations into running code, so as to make things run as fast as possible.  As a result, it falls to programmers to explicitly provide error checking.  Unfortunately, people often neglect to do this, even in critical applications where speed is not a constraint.  Their programs lead fast and dangerous lives.  For example, the notorious "Worm" that paralyzed the Internet in 1988 exploited the UNIX(tm) operating system's failure to check whether the input buffer has overflowed in the finger daemon.  (See Spafford 1989.)

[fn:338] Of course, with either the interpretation or the compilation strategy we must also implement for the new machine storage allocation, input and output, and all the various operations that we took as "primitive" in our discussion of the evaluator and compiler.  One strategy for minimizing work here is to write as many of these operations as possible in Lisp and then compile them for the new machine.  Ultimately, everything reduces to a small kernel (such as garbage collection and the mechanism for applying actual machine primitives) that is hand-coded for the new machine.

[fn:339] This strategy leads to amusing tests of correctness of the compiler, such as checking whether the compilation of a program on the new machine, using the compiled compiler, is identical with the compilation of the program on the original Lisp system.  Tracking down the source of differences is fun but often frustrating, because the results are extremely sensitive to minuscule details.
